{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8d71422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import pdfplumber\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8d21c0",
   "metadata": {},
   "source": [
    "### [!] Show modeling in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "547b808d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME               ID              SIZE      MODIFIED    \n",
      "llama3.2:latest    a80c4f17acd5    2.0 GB    5 hours ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4475da7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'เงินประกันเป็นหนึ่งในส่วนประกอบสำคัญของการลงทุน ซึ่งเป็นการลงทุนในหุ้นหรือหลักทรัพย์อื่นๆ โดยมีการรับประกันว่าจะได้รับรายได้จากการลงทุน โดยทั่วไป เงินประกันจะถูกกำหนดไว้ขึ้นล่วงหน้า และจะได้รับการชำระเงินโดยมีจำนวนการชำระเท่ากันกับส่วนแบ่งสิทธิในหลักทรัพย์ที่เป็นรูปธรรม'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"เงินประกันคืออะไร\"\n",
    "res = ollama.chat(model=\"llama3.2\"\n",
    "                  ,messages=[{\"role\":\"user\",\"content\":prompt}])\n",
    "res[\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ece59d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:/Program Files (x86)/Tesseract-OCR/tesseract.exe\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1229db1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":24l0.08406vl [CS.HC] 10 Oct 2024\n",
      "\n",
      "a1‘Xiv\n",
      "\n",
      "Promptly Yours? A Human Subject Study on\n",
      "Prompt Inference in AI-Generated Art\n",
      "\n",
      "Khoi Trinhl, Joseph Spracklenz, Raveen Wijewickramaz,\n",
      "Bimal Viswanath3, Murtuza Jadliwalaz, Anindya Maiti‘\n",
      "khoitrinh @ 0u.edu, joseph. spracklen @my. utsa.edu, raveen. wtjewickrama @ utra. edu,\n",
      "\n",
      "vbimal @ vzﬁedu, murtuzajadliwala @ utsa. edu, am @014. edu\n",
      "\n",
      "1 University of Oklahoma\n",
      "2 University of Texas at San Antonio\n",
      "3 Virginia Tech\n",
      "\n",
      "Abstract\n",
      "\n",
      "The emerging ﬁeld of AI-generated art has witnessed the rise\n",
      "of prompt marketplaces, where creators can purchase, sell,\n",
      "or share prompts to generate unique artworks. These market-\n",
      "places often assert ownership over prompts, claiming them\n",
      "as intellectual property. This paper investigates whether con-\n",
      "cealed prompts sold on prompt marketplaces can be consid-\n",
      "ered as secure intellectual property, given that humans and AI\n",
      "tools may be able to approximately infer the prompts based\n",
      "on publicly advertised sample images accompanying each\n",
      "prompt on sale. Speciﬁcally, our survey aims to as. ss (i) how\n",
      "accurately can humans infer the original prompt solely by ex-\n",
      "amining an AI-generated image, with the goal of generating\n",
      "images similar to the original image, and (ii) the possibility\n",
      "of improving upon individual human and AI prompt infer-\n",
      "ences by crafting human—AI combined prompts with the help\n",
      "of a large language model. Although previous research has\n",
      "explored the use of AI and machine learning for prompt infer-\n",
      "ence (and also to protect against it), we are the ﬁrst to include\n",
      "humans in the loop. Our ﬁndings indicate that while humans\n",
      "and human-AI collaborations can infer prompts and generate\n",
      "similar images with high accuracy, they are not as successful\n",
      "as using the original prompt.\n",
      "\n",
      " \n",
      "\n",
      "1 Introduction\n",
      "\n",
      "Artiﬁcial Intelligence (AI) has made remarkable strides in\n",
      "the domain of creative and artistic expression, enabling an\n",
      "easy and automated process for everyone to generate visu-\n",
      "ally captivating and conceptually intriguing art work. Cen-\n",
      "tral to this creation process of AI-generated art and images\n",
      "are deep learning based text-to-image (txt:2img) mod-\n",
      "els for image generation that utilize text prompts as in-\n",
      "put (instructions) from users to generate unique and diverse\n",
      "image/art outputs. Some of the most popular open-source\n",
      "or commercially-available examples of such t:xt:2img\n",
      "models include Midjourney (MidJoumey (web), DALL-E\n",
      "2 (Ramesh et al. 2021), Stable Diffusion (Rombach et al.\n",
      "2022) and GLIDE (Nichol et al. 2022).\n",
      "\n",
      "At a high level, these models have two main components\n",
      "— a Language Model (e.g., CLIP‘) and a Generative im-\n",
      "age model (eg, Stable Diffusion (Rombach et al. 2022)).\n",
      "\n",
      "Copyright © 2025, A‘ 'ation for the Advancement of Artiﬁcial\n",
      "Intelligence (www rg). All rights reserved.\n",
      "Ihttps://openai.com/research/clip\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "The language model converts a given text prompt to a la-\n",
      "tent representation, which is then used to condition the gen-\n",
      "erative image model to produce an image that captures the\n",
      "prompt description. Furthermore, these models are trained\n",
      "on vast datasets of text-image pairs, allowing them to under-\n",
      "stand and render complex visual concepts from textual de-\n",
      "scriptions with remarkable accuracy and creativity. For ex-\n",
      "ample, Stable Diffusion was trained on the publicly avail-\n",
      "able LAION-5B dataset, containing 5 billion image-caption\n",
      "pairs, derived from data scraped from the Internet.\n",
      "\n",
      "Text prompts serve as critical input instructions to\n",
      "txt2img models for generating high-quality text-\n",
      "conditioned images and it is non-trivial to deduce an\n",
      "appropriate prompt for the desired image, often requiring\n",
      "creativity and trial-and-error (Wang et al. 2023). This\n",
      "has resulted in the emergence of new prompt engineering\n",
      "jobs and prompt marketplaces for AI-generated art, where\n",
      "prompt engineers, artists, and enthusiasts can exchange\n",
      "and sell prompts that can generate custom high-quality\n",
      "art. Given the importance of selecting the right prompt for\n",
      "generating a desired image and the non-triviality in deter-\n",
      "mining one, these text prompts are often treated as protected\n",
      "information by their creators. Prompt marketplaces often\n",
      "claim intellectual property rights over the prompts, asserting\n",
      "that they are valuable and original creations worthy of legal\n",
      "protection (PromptBase; Promptrrio). Given the protected\n",
      "status of (input) text prompts, two research questions arise\n",
      "that are largely unexplored thus far: (i) how accurately can\n",
      "humans infer the input text prompt by just viewing the\n",
      "(AI-generated) image generated from that prompt? and (ii)\n",
      "can AI tools assist humans in more accurately inferring text\n",
      "prompts of a target (AI-generated) image?\n",
      "\n",
      "To address these research questions, our study employs a\n",
      "human subject survey to assess how accurately users (par-\n",
      "ticipants) can infer prompts by just visually examining the\n",
      "AI-generated images. Our survey results provides valuable\n",
      "insights on how users’ prediction (of prompts) performance,\n",
      "measured using well-deﬁned metrics, varies with different\n",
      "prompt-related attributes and varying user demography and\n",
      "backgrounds. After combining responses from the survey\n",
      "(taken by human subject participants) with responses from\n",
      "an AI-based prompt inference model, we further re-evaluate\n",
      "and compare the overall inference accuracy in this human-\n",
      "\n",
      "\n",
      "Al collaborative setting. Our results show that although both\n",
      "human and combined human-Al efforts can accurately in-\n",
      "fer prompts and recreate images to a great extent, they fall\n",
      "short of the effectiveness achieved with the original prompts.\n",
      "Consequently, marketplaces for selling prompts and cre-\n",
      "ators who offer prompts for Al-generated art can continue\n",
      "to maintain a viable business model.\n",
      "\n",
      "2 Related Work and Research Goals\n",
      "\n",
      "2.1 Prompt Inference in AI Art\n",
      "\n",
      "Previous research in the literature has primarily ex-\n",
      "plored Al/ML-based inference techniques to deduce (infer)\n",
      "prompts and has also proposed Al/ML-based methods to\n",
      "safeguard against such kind of prompt inference threats.\n",
      "Several prior works have employed machine learning al-\n",
      "gorithms to reverse-engineer the prompts used in the cre-\n",
      "ation of artworks (CLIP Interrogator; Shen et al. 2023; Wu\n",
      "et al. 2022; Li et al. 2022), highlighting the potential vul-\n",
      "nerability of prompt concealment. On the other hand, ef-\n",
      "forts have also been made to develop protective measures\n",
      "to secure the prompts from unauthorized access or replica-\n",
      "tion (Shen et al. 2023; Struppek, Hintersdorf, and Kersting\n",
      "2022; Zhai et al. 2023). These efforts involve strategies to\n",
      "thwart prompt inference, backdoor injections, and data poi-\n",
      "soning, implementing rigorous dataset inspections, and em-\n",
      "ploying anomaly detection. However, if humans are able to\n",
      "infer prompts with a high degree of accuracy, the effective-\n",
      "ness of these protective measures against prompt inference\n",
      "may be called into question. Furthermore, it is possible that\n",
      "Al-assisted prompt inference and prompt inference by hu-\n",
      "mans can be effectively combined to signiﬁcantly improve\n",
      "the overall inference accuracy, and needs to be further stud-\n",
      "ied. While there are recent studies investigating the effec-\n",
      "tiveness of human-Al collaboration leading to more creative\n",
      "artworks (Lyu et al. 2022; Brade et al. 2023), currently we\n",
      "have little understanding of how a similar human-Al collab-\n",
      "oration may work towards prompt inference of AI- generated\n",
      "art.\n",
      "\n",
      "2.2 Challenges in Prompt Inference\n",
      "\n",
      "Inferring prompts of Al-generated images is a complex task\n",
      "that both humans and Al models (CLIP Interrogator; Mid-\n",
      "Journey (web; Shen et al. 2023) can ﬁnd challenging due to\n",
      "the complexities in visual content and the subtleties of lan-\n",
      "guage. Figure l exempliﬁes this by illustrating how varying\n",
      "the modiﬁers (in this case, “pixel art” and “dark colors”) for\n",
      "a consistent subject (a cat) can lead to vastly different visual\n",
      "outcomes when combined with different modiﬁers. Even the\n",
      "addition of a single modiﬁer can dramatically alter the re-\n",
      "sulting image, demonstrating how each element in a prompt\n",
      "contributes to the generated image. Conversely, omitting a\n",
      "crucial modiﬁer could lead to a visual representation that\n",
      "misses the depth or context intended. When humans try to\n",
      "deduce the prompts for Al-generated images, they often rely\n",
      "on their subjective interpretation and understanding of the\n",
      "visual content, which can lead to varied conclusions on the\n",
      "subjects and modiﬁers used in the image generation. The un-\n",
      "derstanding of how humans may infer prompts, speciﬁcally\n",
      "\n",
      "cat, pixel art,\n",
      "dark colors\n",
      "\n",
      " \n",
      "\n",
      "Figure 1: Image generations using SDXL with prompts con-\n",
      "taining the same subject (cat) and different combinations of\n",
      "two modiﬁers (pixel art and dark colors).\n",
      "\n",
      "the subjects and modiﬁers used in Al-generated art, with or\n",
      "without the aid of Al-based prompt inference tools, remains\n",
      "an unexplored research area and is the focus of this work.\n",
      "\n",
      "2.3 Research Goals\n",
      "\n",
      "Given the above intricacies in human and Al prompt infer-\n",
      "ences, it is not trivial to assess whether concealed prompts\n",
      "sold on prompt marketplaces can be considered as secure\n",
      "intellectual property, or are they vulnerable to prompt infer-\n",
      "ence and replication. Moreover, there is a lack of a clearly\n",
      "deﬁned measurement based on which a prompt inference\n",
      "can be deemed successful, especially for human prompt in-\n",
      "ference. To address this gap, our research goals are as fol-\n",
      "lows:\n",
      "\n",
      "- Determining the accuracy with which individuals can infer\n",
      "the original prompts from images created by a txt2 img\n",
      "model, aiming to reproduce similar images. This is accom-\n",
      "plished by designing and conducting a comprehensive hu-\n",
      "man participant survey as outlined in Section 3 and Sec-\n",
      "tion 4. Additionally, exploring whether a participant with\n",
      "an art background (eg. an art major in college, a graphic\n",
      "designer) would have an advantage over one without such\n",
      "a background. Evaluation is to be based on similarity be-\n",
      "tween the original image and the images reproduced using\n",
      "the inferred prompt.\n",
      "\n",
      "- Determining any improvement in prompt inference accu-\n",
      "racy when human inferred prompts are combined with\n",
      "Al inferred ones, aiming to further improve the similar-\n",
      "ity of the reproduced images to the original images. This\n",
      "is achieved by integrating the survey responses of the hu-\n",
      "man participants and Al inferred prompts as discussed in\n",
      "Section 4.3. Evaluation is to be based on similarity be-\n",
      "tween the original image and the images reproduced using\n",
      "the combined prompt.\n",
      "\n",
      "- Establishing robust thresholds of metrics for measuring\n",
      "the success of both standalone and combined human-Al\n",
      "\n",
      "inferences, as detailed in Section 4.2, ensuring a clear\n",
      "framework for assessment.\n",
      "\n",
      "3 Survey Design and Participants\n",
      "3.1 Prompt and Image Datasets\n",
      "\n",
      "The images presented to the participants were generated\n",
      "from two distinct types of prompts: Controlled and Uncon-\n",
      "trolled. Controlled prompts serve as a baseline to gauge\n",
      "\n",
      "\n",
      "participants’ ability to recognize common subjects and\n",
      "modiﬁers, allowing us to assess other variables associ-\n",
      "ated with prompt inference, such as any differences be-\n",
      "tween txt2img models and demographic-based differ-\n",
      "ences. Conversely, uncontrolled prompts feature a much\n",
      "more diverse mix of subjects and modiﬁers and is used\n",
      "to construct more comprehensive and challenging inference\n",
      "tasks for participants.\n",
      "\n",
      "Controlled Dataset. The controlled prompts set was con-\n",
      "structed through an analysis of common subjects discovered\n",
      "on the dynamically changing homepage of Lexicaz, a pop-\n",
      "ular prompt and A1 art sharing platform. We selected Lex-\n",
      "ica for this data gathering task as they have web crawling-\n",
      "friendly terms of service. Our Selenium script identiﬁed\n",
      "100579 different images and accompanying prompts by\n",
      "their speciﬁc HTML elements. Subsequent processing with\n",
      ".rpaCy, a Natural Language Processing (NLP) framework,\n",
      "enabled us to distill the subjects from these prompts. The\n",
      "ﬁve most frequently occurring subjects identiﬁed were man,\n",
      "woman, astrommt, cat, and robot. In parallel with subject\n",
      "selection, we curated a set of modiﬁers by referencing a\n",
      "Midjourney styles and keywords repository (Wulfken (web).\n",
      "The modiﬁers spanned various categories such as themes/-\n",
      "genres, lighting, drawing and art medium, perspective, emo-\n",
      "tion/mood, colors and palettes, geography and culture, ren-\n",
      "dering/shading style, culminating in a list of 121 modiﬁers\n",
      "(details in Appendix E.5). These subjects and modiﬁers were\n",
      "then randomly sampled and combined, ranging from one\n",
      "to ﬁve modiﬁers per subject, to formulate 100 controlled\n",
      "prompts, 20 per subject (Appendix E.6). These 100 prompts\n",
      "were then used to generated images for Parts 1 and 111 of\n",
      "our survey (3), 25 for each of the four selected t:xt:2img\n",
      "models.\n",
      "\n",
      "Uncontrolled Dataset. The uncontrolled prompts set was\n",
      "constructed through a random sampling of 100 whole/com-\n",
      "plete prompts discovered on PromptHero (a platform for dis-\n",
      "covering and sharing A1-generated art and text prompts).\n",
      "There were no restrictions on what subject and modiﬁers\n",
      "appeared in these prompts, as long as they were non-explicit\n",
      "in nature. These 100 prompts were then used to generate im-\n",
      "ages for Part 11 of our survey, 25 for each of the four selected\n",
      "txt 2 img models.\n",
      "\n",
      "3.2 Analyzed txt2img Models\n",
      "\n",
      "Our study focuses on four popular txt2img models, in-\n",
      "cluding two base models and two ﬁne-tuned models:\n",
      "MidJourney V5.0 (MidJourney (web).\n",
      "\n",
      "Stable Diffusion XL (SDXL) (Podell et al. 2023; Rom-\n",
      "bach et al. 2022).\n",
      "\n",
      "DreamShaper XL (Podell et al. 2023).\n",
      "Realistic Vision VS (Hugging Face b).\n",
      "\n",
      "At the time we began this study in late 2023, these mod-\n",
      "els had the highest number of prompts being shared or sold\n",
      "on platforms such as PromptHero, Promptrr.io, Prompti A1,\n",
      "\n",
      "Zhttps://lexicaart\n",
      "\n",
      "PromptBase, and CivitA1. More details on these four mod-\n",
      "els and how they were employed in our study can be found\n",
      "in Appendix B.\n",
      "\n",
      "Besides the above, there are other popular txt2img\n",
      "models which we also attempted to include in our study, but\n",
      "could not for various reasons. For instance, OpenA1’s pop-\n",
      "ular DALL-E model utilizes a modiﬁed version of the GP'T\n",
      "model by adapting its transformer-based architecture for im-\n",
      "age generation (Ramesh et al. 2021). However, DALL-E was\n",
      "omitted from our study because we were unable to lock in\n",
      "a speciﬁc version of the model for image creation. We were\n",
      "concerned that DALL-E’s updates during the time-frame of\n",
      "our study could affect the consistency of the images pro-\n",
      "duced initially and those generated later for comparison.\n",
      "\n",
      "For our user study to assess the ability of humans to in-\n",
      "fer prompts from A1-generated images, we design and im-\n",
      "plement a custom survey tool. This tool dynamically loads\n",
      "and displays images (and its metadata) together with related\n",
      "question(s), and can capture participant responses to these\n",
      "questions (see Figure 22 and Appendices E.1 to E.3). We\n",
      "collected responses from 230 participants using this custom\n",
      "tool, of which 59 were recruited from two public university\n",
      "campuses (in the US), while the remaining 171 participants\n",
      "were recruited via the Amazon Mechanical Turk (MTurk)\n",
      "platform. This study has been approved by the Institutional\n",
      "Review Boards (lRBs) of the institutions involved. Before\n",
      "completing the main survey task, participants completed a\n",
      "preliminary questionnaire (Figure 16 in Appendix E) that\n",
      "requested demographic information and self-reported famil-\n",
      "iarity with generative A1 tools. Below, we ﬁrst summarize\n",
      "ﬁndings from the preliminary survey, followed by a detailed\n",
      "description of the main survey tasks completed by the par-\n",
      "ticipants.\n",
      "\n",
      "3.3 Pre-Survey: Demographics\n",
      "\n",
      "30.9% of the participants in our study are in the age group of\n",
      "35-44 years, followed by 26.5% in 25-34 years and 20.4%\n",
      "in 18-24 years. A small number of remaining participants\n",
      "are 45 years or older. Figure 17a and Figure 17b (in Ap-\n",
      "pendix C) illustrate the age and gender distribution of our\n",
      "participants, respectively. Most of our participants are male\n",
      "(59.1%), with the remaining females (38. 7%) and those who\n",
      "identify as other gender categories (2.2%). 15.65% partici-\n",
      "pants reported having an arts background or education, in-\n",
      "dicating a certain level of expertise or familiarity with cre-\n",
      "ative ﬁelds, while the remaining 84. 35% did not have such\n",
      "a background, representing a broader range of occupations\n",
      "and experiences.\n",
      "\n",
      "3.4 Pre-Survey: Familiarity with Generative AI\n",
      "\n",
      "We separated the level of familiarity of our participants\n",
      "with various generative A1 tools into four distinct categories,\n",
      "namely, familiarity with the text, image, audio, and video\n",
      "generative A1 tools (Figure 2). The predominant level of\n",
      "familiarity for text and audio tools is “Slightly Familiar,”\n",
      "(41.7% for text and 32.6% for audio), suggesting a moderate\n",
      "acquaintance with these technologies. For image generation\n",
      "tools, the results indicate an evenly spread level of famil-\n",
      "iarity, with both “Slightly Familiar” and “Somewhat Famil-\n",
      "\n",
      "\n",
      "2! 2! :3\n",
      "Video : 3  \n",
      "V M H N\n",
      "2! 2! :3\n",
      "Audio '2 W. \"2 §\n",
      "5 3 .1 V\"\n",
      "5 § § §\n",
      "\"\"“-1‘ w e\" u. ..\n",
      ".-. .. .. H\n",
      "2! :3 :3\n",
      "rm § : 3 :\n",
      "Q Q NI N\n",
      "0 20 40 60 30 100\n",
      "\n",
      "Percentage Participants\n",
      "\n",
      "Not at All Familiar\n",
      "Slightly Familiar\n",
      "\n",
      "Somewhat Familiar\n",
      "Very Familiar\n",
      "\n",
      "Figure 2: Participants’ familiarity levels with different gen-\n",
      "erative AI tools.\n",
      "\n",
      "iar” categories capturing the largest proportions at roughly\n",
      "350% each. For video generation tools, both “Nat At All\n",
      "Familiar” and “Slightly Familiar” categories capture the\n",
      "largest proportions at 43.0% and 34.3%, respectively. “Very\n",
      "Familiar” holds smaller shares across video and audio tools\n",
      "(4.8% for audio and 7.0% for video), while for image and\n",
      "text generation tools “Nat atAll Familiar” remains the least\n",
      "represented (4.8% for text and 16.1% for image). This dis-\n",
      "tribution demonstrates that our participants are moderately\n",
      "familiar with a broad range of generative AI tools.\n",
      "\n",
      "3.5 Parts I-III: Prompt Inference\n",
      "\n",
      "Our pool of participants, both recruited on campus and\n",
      "through MechTurk, participated in the main survey, com-\n",
      "prised of three sequentially ordered parts (or phases), where\n",
      "in each part participants were tasked with either selecting\n",
      "or typing-in the most appropriate prompt (comprising of a\n",
      "subject and modiﬁers) for a series of AI-generated images.\n",
      "In Parts I and III, a selection of 100 images using a con-\n",
      "trolled set of subjects and modiﬁers, each methodically pro-\n",
      "duced using the four image generation models outlined in\n",
      "Section 32, were displayed to the participants. Each of these\n",
      "models contributed an equal share of images to the con-\n",
      "trolled dataset. Consistent exposure to all subjects and mod-\n",
      "iﬁers in the controlled set was maintained by appropriately\n",
      "varying the subset of images each of the participants were\n",
      "exposed to.\n",
      "\n",
      "In contrast, Part II expanded the scope of inference by\n",
      "using 100 images from an uncontrolled dataset. These im-\n",
      "ages were generated from popular prompts found on online\n",
      "prompt sharing websites, offering a wide array of subjects\n",
      "and modiﬁers. This uncontrolled set also used the same four\n",
      "image generation models. Detailed information on the sub-\n",
      "jects and modiﬁers selected for the controlled dataset and\n",
      "the composition of the uncontrolled dataset can be found in\n",
      "Section 3.1.\n",
      "\n",
      "Part I: Subject and Modiﬁer Selection in Controlled\n",
      "Dataset. In this part, participants were presented with a se-\n",
      "quence of ﬁve AI-generated images with controlled subjects\n",
      "and modiﬁers (i.e, from the controlled dataset). For each dis-\n",
      "played image, they were required to ﬁrst select the correct\n",
      "\n",
      "subject from the options provided. Next, they had to choose\n",
      "the correct modiﬁer(s) from the given options (between 1 to\n",
      "5 checkbox options), with the possibility of selecting mul-\n",
      "tiple modiﬁers if preferred. For each question, there will be\n",
      "one correct subject, with the rest of the options ﬁlled in ran-\n",
      "domly; at least one and at most ﬁve correct modiﬁers. For\n",
      "questions where there are less than ﬁve correct modiﬁers, the\n",
      "remaining modiﬁers are ﬁlled in randomly. Figure 19 shows\n",
      "an example of a question for this part. The goal of this task\n",
      "was to measure the participants’ ability to discern and match\n",
      "the given images to their corresponding prompts accurately.\n",
      "\n",
      "Part II: Prompt Creation in Uncontrolled Dataset. This\n",
      "second part challenged participants with another ﬁve AI-\n",
      "generated images, but this time the prompts were uncon-\n",
      "trolled, ie., no pre-selected options were provided for both\n",
      "subjects and modiﬁers. In this task, for each image partic-\n",
      "ipants had to type what they believed to be the most ﬁtting\n",
      "subject and modiﬁer(s) as a complete sentence. Figure 20 (in\n",
      "(in Appendix E2) shows an example of a question for this\n",
      "part. This open-ended task assessed their creative inference\n",
      "abilities and how they translated their interpretation of the\n",
      "images into descriptive prompts.\n",
      "\n",
      "Part III: Prompt Creation in Controlled Dataset. In this\n",
      "ﬁnal task, participants were shown yet another set of ﬁve AI-\n",
      "generated images. Similar to Part II, they were instructed to\n",
      "generate sentence-like prompts for these images. However,\n",
      "in Part III the images were again based on controlled sub-\n",
      "jects and modiﬁers (i.e., from the controlled dataset). Fig-\n",
      "ure 21 (in Appendix E.3) shows an example ofa question for\n",
      "this part. The responses of the participants were then com-\n",
      "pared with the actual prompts and images displayed to them,\n",
      "providing a measure of how well the participants could infer\n",
      "and replicate the structured prompts that were initially used\n",
      "to generate the images displayed.\n",
      "\n",
      "3.6 Part IV: Rating Similarity\n",
      "\n",
      "In addition to parts I through III, a subset (all of our 171\n",
      "MTurk participants) were asked to take an additional part.\n",
      "In this last part (Part IV) of the study, participants rated the\n",
      "similarity between pairs of images, ane rhnwn to prior par-\n",
      "ticipants and one generated using their (priarparticipant.r’)\n",
      "prompts. Speciﬁcally, participants were presented with pairs\n",
      "of images, similar to the ones depicted in Figure 22 (in Ap-\n",
      "pendix E.4). Each pair consisted of an original image from\n",
      "Part III and a newly generated image based on a prompt\n",
      "submitted by one of the ﬁrst 25 (on campus) participants.\n",
      "The participants’ task for each image pair was to assess and\n",
      "rate the similarity between the two images. They were pro-\n",
      "vided with a Likert scale ranging from “Not at All Similar”\n",
      "to “Very Similar”. This allowed participants to express their\n",
      "perceived degree of similarity, taking into account factors\n",
      "such as color scheme, composition, and subject representa-\n",
      "tion. After making their selection, they would proceed to the\n",
      "next comparison until they had evaluated all ﬁve pairs as-\n",
      "signed to them.\n",
      "\n",
      "\n",
      "3.7 Survey Responses\n",
      "\n",
      "In total, 230 on campus and MTurk participants took our\n",
      "survey between December 2023 and March 2024. The re-\n",
      "sponses collected consists of subject and modiﬁer selections\n",
      "from a ﬁxed set of options in Part I, and whole prompts in\n",
      "Parts II and III of the survey. After completion of the sur-\n",
      "vey and elimination of invalid responses, we recorded 1078\n",
      "responses for Part I, 1 145 responses for Part II, and 1141 re-\n",
      "sponses for Part III. In addition, the participants who were\n",
      "selected for Part IV of our study gave us 855 responses. In\n",
      "Parts II and III, responses that were incoherent or irrelevant,\n",
      "such as the failure to specify at least one subject or modiﬁer,\n",
      "were also excluded.\n",
      "\n",
      "4 Experimental Setup\n",
      "4.1 Metrics\n",
      "\n",
      "Next, we deﬁne the metrics that we employ to assess the\n",
      "prompt inference accuracy of the survey participants in our\n",
      "experiments. By means of these metrics, we aim to compare\n",
      "and compute the discrepancy between the original prompts\n",
      "and participants’ responses (inferred prompts), and between\n",
      "the original image (displayed to the participants) and images\n",
      "generated from their (inferred) prompts. A detailed descrip-\n",
      "tion of these metrics can be found in Appendix D. These\n",
      "metrics provide a comprehensive assessment of both objec-\n",
      "tive (e.g., image hash, perceptual similarity) and subjective\n",
      "(e.g., surveyed similarity ratings) aspects, ensuring a well-\n",
      "rounded evaluation of the participants’ performance in the\n",
      "study.\n",
      "\n",
      "- MSQ Scores (Survey Part I). Score between 0 (no cor-\n",
      "rect selection) to 2 (all correct selections) for each ques-\n",
      "tion to evaluate multiple selection questions (MSQs).\n",
      "\n",
      "- Image Hash3. Hamming distance between the hashes of\n",
      "two images. A lower image hash score (difference) im-\n",
      "plies that the two images are similar, and vice versa.\n",
      "\n",
      "- Perceptual Similarity (Zhang et al. 2018). A lower per-\n",
      "ceptual similarity score implies that the two images are\n",
      "similar, and vice versa.\n",
      "\n",
      "- Image Embedding Similarity (CLIP Score) (Wang,\n",
      "Chan, and Loy 2023). CLIP score between an image\n",
      "and a prompt (text) where a higher score indicates greater\n",
      "relevance or similarity. We employ two state-of-the-art\n",
      "CLIP models in our evaluation, OpenAI’s ViT-L/ 14 Trans-\n",
      "former‘ (L14) and ViT-B/32 Transformers (B32).\n",
      "\n",
      "- Text Embedding Similarity (Semantic Similar-\n",
      "ity) (Reimers and Gurevych 2019). A higher semantic\n",
      "similarity implies the two prompts are similar, and vice\n",
      "versa.\n",
      "\n",
      "- Surveyed Similarity Rating (from Survey Part IV). Par-\n",
      "ticipants in Part IV of the study rated pairs of images on\n",
      "a Likert scale consisting of “Not at All Similar”, “Slightly\n",
      "Similar”, “Somewhat Similar”, and “Very Similar.”\n",
      "\n",
      "3https://pypi.org/project/ImageHash/\n",
      "4https://huggingface.co/openai/clip- vit- large- patch 14\n",
      "5https://huggingface.co/openai/clip— vit-base— patch32\n",
      "\n",
      "4.2 Quantifying Successful Inferences\n",
      "\n",
      "Given our study’s focus on assessing the accuracy of human\n",
      "prompt inference is ultimately in understanding their ability\n",
      "to generate images resembling those presented to them, we\n",
      "now establish a quantiﬁable measure of success threshold\n",
      "based on the above metrics, speciﬁcally image hash, percep-\n",
      "tual similarity, and CLIP scores. Considering the inherent\n",
      "variability in txt2img generations for identical prompts,\n",
      "achieving perfect scores (1 for CLIP score, 0 for image\n",
      "hash, and perceptual similarity) is implausible even with\n",
      "completely accurate prompt inference, as illustrated by Fig-\n",
      "ures 18a and 18b scores.\n",
      "\n",
      "Therefore, a more appropriate threshold for gauging suc-\n",
      "cessful inference involves analyzing the range of scores for\n",
      "images generated from the same prompt across different\n",
      "instances. Accordingly, to determine an effective success\n",
      "threshold, we assessed the scores generated from identical\n",
      "prompts across multiple image creations, using our 100 con-\n",
      "trolled dataset prompts (Appendix E.6). For each of these\n",
      "prompts, we generated two different images using SDXL,\n",
      "and calculated the image hash, perceptual similarity, and\n",
      "B32 and L14 CLIP scores. After calculating the averages\n",
      "of these scores, we determined the success thresholds, de-\n",
      "noted by 9, as follows: ﬁlm}, : 26.83 for image hash,\n",
      "9px : 0.575 for perceptual similarity, and (9332 : 0.875 and\n",
      "9L1./1 : 0.851 for B32 and L14 CLIP scores, respectively.\n",
      "Participants whose inferred prompts are able to generate im-\n",
      "ages close to or better than these thresholds can be deemed\n",
      "successful in their inference.\n",
      "\n",
      "4.3 Human-AI Combined Inference\n",
      "\n",
      "We now detail our experimentation on the effectiveness of\n",
      "combining human inferred prompts with those produced by\n",
      "AI models towards accurately recreating AI-generated art.\n",
      "Speciﬁcally, we utilize prompt responses collected in Part III\n",
      "of our survey, which involved controlled dataset prompts,\n",
      "and AI-generated prompts obtained through the CLIP Inter-\n",
      "rogator, a model that analyzes an image and generates de-\n",
      "scriptive text prompts by leveraging OpenAI’s CLIP model\n",
      "to match images and text representations (CLIP Interroga-\n",
      "tor). These human and AI prompt pairs are then consol-\n",
      "idated into one combined prompt for comparative evalua-\n",
      "tion against only human inferred prompt, CLIP Interrogator\n",
      "prompt, and the success threshold, in Section 5.3.\n",
      "\n",
      "To construct a combined prompt from each pair of hu-\n",
      "man prompt and corresponding CLIP Interrogator prompt,\n",
      "we employ a large language model such as GP'T-4 (Ope-\n",
      "nAI 2023), and instruct it to merge the two prompts into a\n",
      "succinct new prompt of up to 25 words without adding any\n",
      "extraneous information. An example of this process is illus-\n",
      "trated below, where the ﬁrst prompt was from a participant\n",
      "and the second prompt was generated by CLIP Interrogator:\n",
      "\n",
      "\n",
      "Instruction: Combine these two prompts into a new\n",
      "prompt of 25 words, without any extra information added:\n",
      "1. man dressed in steampunk in a steampunk factory\n",
      "\n",
      "2. a man in a steampunk suit and top hat standing in front\n",
      "of a giant clock with gears, Bastien L. Deharme, steam-\n",
      "\n",
      "punk, a character portrait, fantasy art\n",
      "\n",
      "GPT-4 Response: a man in a steampunk suit and top hat\n",
      "stands in a factory, surrounded by giant clocks and gears,\n",
      "embodying a fantasy art portrait.\n",
      "\n",
      " \n",
      "\n",
      "The combined prompts were capped at 25 words to pre-\n",
      "vent GPT-4 from inadvertently introducing additional key-\n",
      "words when given unrestricted length. Employing a large\n",
      "language model was considered more suitable than merely\n",
      "concatenating the two prompts to avoid repeating subjects\n",
      "and modiﬁers. Such repetitions could skew the txt2‘1mg\n",
      "generations by inadvertently emphasizing repeated subjects\n",
      "and modiﬁers over others.\n",
      "\n",
      "5 Results and Analysis\n",
      "5.1 Subject and Modiﬁer Selection Evaluation\n",
      "\n",
      "In Figure 3, the y-axis illustrates the Part I MSQ score\n",
      "distributions over the speciﬁed categories (models, sub-\n",
      "jects, and demographic), reﬂecting participants’ ability to\n",
      "match AI-generated images with their corresponding sub-\n",
      "jects and modiﬁers where a higher score indicate a more ac-\n",
      "curate prompt matching. As seen in Figure 3a, Midjoumey-\n",
      "generated images showed a relatively high score concentra-\n",
      "tion towards the upper end, with a median score of 1.5 and\n",
      "an interquartile range (IQR) ranging from 1.33 to 1.9. Real-\n",
      "istic Vision 5’s generations demonstrated a median of 1.38\n",
      "and an IQR from 1.2 to 1.6. DreamShaper XL’s images dis-\n",
      "played a median of 1.49 and an IQR from 1.28 to 1.75, sug-\n",
      "gesting slightly more consistency than Realistic Vision 5.\n",
      "Stable Diffusion XL matched Midjourney in median score\n",
      "but had a narrower IQR from 1.3 to 1.67.\n",
      "\n",
      "Analyzing scores by subject in Figure 3b, cat-themed im-\n",
      "ages outperformed others with a median of 1.5 and an IQR\n",
      "from 1.33 to 1.84, while astronaut images lagged behind\n",
      "with a median of 1.33 and an IQR from 1.23 to 1.58. Next,\n",
      "dissecting the impact of an art background, Figure 3c shows\n",
      "that participants with art-related education or employment\n",
      "achieved slightly higher median scores at 1.5 and an IQR\n",
      "of 1.32 to 1.61 compared to those without, who had a me-\n",
      "dian of 1.48 and a similar IQR but with a broader range and\n",
      "more outliers. Participant demographics split by recruitment\n",
      "source, shown in Figure 3d, highlight differences in scoring\n",
      "patterns. MTurk workers registered a median score of 1.48\n",
      "and an IQR of 1.31 to 1.6, while university-recruited partic-\n",
      "ipants posted a slightly higher median of 1.54 with an IQR\n",
      "from 1.42 to 1.67. Notably, the MTurk group exhibited a\n",
      "broader score range and more outliers, indicating variances\n",
      "in scoring behavior between the two groups. These results\n",
      "broadly imply that when given with options, participants ac-\n",
      "curately identiﬁed subjects and modiﬁers in AI-generated\n",
      "images, achieving high scores across different models.\n",
      "\n",
      "2.00\n",
      "1.15\n",
      "1.50\n",
      "E\n",
      ",1.25\n",
      "u\n",
      "\"-1.00\n",
      "3075\n",
      "E . L,\n",
      "0.50 e\n",
      "0.25 3 3\n",
      "0.00 o 0\n",
      "\n",
      "  \n",
      "\n",
      "nream- Mid- R\n",
      "\n",
      " \n",
      "\n",
      "snaper inur- XL rnan at m- wo- astro-\n",
      "XL ney um rnan naut\n",
      "(3) (b)\n",
      "2 0 7, 2 0 7,\n",
      "1 a ** 1 a ——\n",
      "\n",
      ">-\n",
      "m\n",
      "\n",
      "   \n",
      "\n",
      "Average Msn Score\n",
      "2'‘ E\" . .\n",
      "h m\n",
      "\n",
      "Average Msn Score\n",
      "\n",
      ".4\n",
      "h\n",
      "\n",
      "1.2 1.2 ——\n",
      "\n",
      "1.0 __ 1.0\n",
      "\n",
      "0 a 3 . 0 a . 3\n",
      "Non-Art Art University MechTurk\n",
      "\n",
      "(C) (d)\n",
      "\n",
      "Figure 3: Analysis of multiple answer question (MSQ) re-\n",
      "sponses from Part I, aimed at identifying disparities in (a)\n",
      "four distinct txt2img models, (b) various subjects de-\n",
      "picted in the images, (c) the impact of participants’ arts\n",
      "background, and (d) variations attributable to recruitment\n",
      "sources.\n",
      "\n",
      "5.2 Human Inference Evaluation\n",
      "\n",
      "We next utilize metrics such as the image hash, percep-\n",
      "tual similarity, CLIP score, semantic similarity, and survey\n",
      "similarity ratings (outlined in Section 4.1) as applicable, to\n",
      "evaluate various factors affecting human inference accuracy.\n",
      "These metrics have ranges of 0 to 1 for CLIP score, semantic\n",
      "similarity, and perceptual similarity; and 0 to 64 for image\n",
      "hash score.\n",
      "\n",
      "Models. Both the uncontrolled and controlled datasets\n",
      "(from Parts II and III, respectively) serve as our primary\n",
      "data sources for this analysis where we compare the images\n",
      "generated from prompts submitted by the participants using\n",
      "our four selected txt2 img models. These images are com-\n",
      "pared to the original image from the corresponding correct\n",
      "prompt using the the similarity metrics.\n",
      "\n",
      "Figure 4 shows the distribution of semantic similarity and\n",
      "CLIP scores obtained for the participant responses for sur-\n",
      "vey Parts II and III. For the controlled dataset (Figure 4a), se-\n",
      "mantic similarity (a higher value represents a higher similar-\n",
      "ity between the original and the participant inferred prompt)\n",
      "reveals that the L14 model falls short of the B32 model’s\n",
      "performance, with a notable difference in median scores\n",
      "(25.687% lower on average for L14). This pattern persists\n",
      "in the CLIP scores (where a higher value represents a higher\n",
      "similarity between the original and the inferred prompt gen-\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "   \n",
      "\n",
      "0.a ' 0.a\n",
      ".. 0.5 .. 0.5\n",
      "L L\n",
      "3 3\n",
      "\"' 0.4 ,, 3 ‘>0 0 V‘ 0.4 8\n",
      "0.2 ° ° 0.2 g\n",
      "Q\n",
      "0.0 0.0\n",
      "Dr:am- Mid- R: isti: SD Dr:am- Mid- Realistic SD\n",
      "shapar inur- ans XL shapar inur- Visinn5 XL\n",
      "XL nay XL nay\n",
      "\n",
      "(3) (b)\n",
      "1 semantic rity B32 1 CLIP Score 332\n",
      "1 semantic rity L14 1 CLIP Score L14\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "Figure 4: Semantic similarity and CLIP score for different\n",
      "txt2img models, in (a) controlled dataset, and (b) uncon-\n",
      "trolled dataset. Success thresholds are depicted as dashed\n",
      "lines, green dashed line for the L14 model and red dashed\n",
      "line for the B32 model.\n",
      "\n",
      "   \n",
      "\n",
      "‘’ 1.0\n",
      "u. 5” 5\n",
      "0| '2\n",
      "§ 4,, g 0.3\n",
      "VI ,, \n",
      "5 so 3 0.6\n",
      "0\n",
      "°' 20 v ‘S.\n",
      "3 § 3 0.4\n",
      "E 10 5\n",
      "A.\n",
      "0.2 0\n",
      "Dream- Mid- Realistic SD Dream- Mid- Realistic SD\n",
      "shapar inur- Visinn5 xL shapar inur- Visinn5 xL\n",
      "\n",
      "xL nay xL nay\n",
      "\n",
      "Z Controlled Z Uncontrolled\n",
      "\n",
      "Figure 5: Perceptual similarity and image hash scores for\n",
      "images generated using the four different txt 2 img models.\n",
      "\n",
      "erated image), where B32 outperforms L14, emphasizing\n",
      "the latter’s lower median by 5.294% on average. Among the\n",
      "txt2 img models, Midjoumey stands out for its superior se-\n",
      "mantic similarity and CLIP scores, indicating a closer match\n",
      "to the inferred prompts, whereas Realistic Vision 5 consis-\n",
      "tently records the lowest score ranges across these metrics.\n",
      "In the uncontrolled dataset (Figure 4b), we observe a\n",
      "similar trend, with the L14 model lagging behind the B32\n",
      "model in both semantic similarity (the median is 22.06%\n",
      "lower on average) and CLIP scores (the median 5.99% lower\n",
      "on average) (Figure 4b). The majority of the CLIP scores\n",
      "observed fell below the success thresholds (dashed lines),\n",
      "(7L14 : 0.851 and 6332 : 0.875, as denoted by the red and\n",
      "green dashed lines respectively in Figure 4. However, it is\n",
      "noteworthy that 162 images from the Midjourney model out\n",
      "of 2, 286 total images were able to surpass these thresholds\n",
      "under B32 across both controlled and uncontrolled datasets.\n",
      "\n",
      "The perceptual similarity and image hash scores, as\n",
      "\n",
      "100\n",
      "\n",
      "illll\n",
      "\n",
      "0\n",
      "DreamShaper XL Midjourney Realistic Vision 5 SDXL\n",
      "\n",
      "Count\n",
      "\n",
      "— Not at All Similar — Somewhat Similar\n",
      "— siighuy Similar — Very Similar\n",
      "\n",
      "Figure 6: Part IV similarity rating distributions for images\n",
      "generated using the four different txi:2 img models.\n",
      "\n",
      "shown in Figure 5, reveal the inﬂuence of dataset conditions\n",
      "(controlled vs. uncontrolled) on image generation in a some-\n",
      "what counter-intuitive manner. Under controlled conditions,\n",
      "the hash scores are higher, with the median hash score across\n",
      "all models being on average 1.185% higher than that for the\n",
      "uncontrolled dataset (note that a lower image hash scores\n",
      "indicate greater similarity). Similarly, the median percep-\n",
      "tual similarity is higher, although only marginally, at 0.329%\n",
      "on average compared to the uncontrolled dataset (note that\n",
      "a lower perceptual similarity score is more desirable, i.e.,\n",
      "greater similarity). These ﬁndings indicate that a more struc-\n",
      "tured approach to prompt inference results in the generation\n",
      "of images that considerably deviate from the original images\n",
      "compared to uncontrolled prompt inference.\n",
      "\n",
      "In terms of success thresholds, a pattern similar to CLIP\n",
      "scores was observed in the image hash and perceptual sim-\n",
      "ilarity evaluations. The success thresholds are depicted in\n",
      "Section 5.2 and Section 5.2 by red dashed lines. Surveyed\n",
      "similarity ratings (Figure 6) provide direct insights into\n",
      "the perceived accuracy of images generated from human-\n",
      "inferred prompts. DreamShaper XL notably receives a ma-\n",
      "jority of “Somewhat Similar” ratings (87), along with the\n",
      "highest count of “Very Similar” ratings at 75, suggesting that\n",
      "images generated with this model are likely to be more sim-\n",
      "ilar to the corresponding original image.\n",
      "\n",
      "These analyses reveals how different txt2img models\n",
      "perform while interpreting prompts and corresponding im-\n",
      "age generations, emphasizing the challenges in reproducing\n",
      "AI art through prompt inference that aligns with human in-\n",
      "terpretations. Despite certain minor favorable observations,\n",
      "the overall performance across all three types of metrics did\n",
      "not reach the success thresholds, indicating the challenging\n",
      "nature in human prompt inferences.\n",
      "\n",
      "Subjects. We next investigate the inference accuracy for\n",
      "different subjects in prompts using the controlled dataset.\n",
      "Figure 7 illustrates the semantic similarity and CLIP scores\n",
      "(where higher scores indicate a greater match), for ﬁve se-\n",
      "lected subjects. It is observed that the L14 model typically\n",
      "scores lower compared to the B32 model in both metrics.\n",
      "Subjects such as robots and cats achieve high CLIP scores,\n",
      "\n",
      "\n",
      "Score\n",
      "\n",
      "   \n",
      "\n",
      "man cat robot woman astronaut\n",
      "\n",
      "1 semantic ‘milarity B32 1 CLIP score 332\n",
      "1 semantic similarity L14 1 CLIP score L14\n",
      "\n",
      " \n",
      "\n",
      "Figure 7: Semantic similarity and CLIP score for images\n",
      "containing different subjects, in contro led dataset.\n",
      "\n",
      "m\n",
      "a\n",
      "\n",
      "image Hash score\n",
      ".. N w .. u.\n",
      "a a a a a\n",
      " W\n",
      "\n",
      "a\n",
      "\n",
      "man cat ro- wo- as- ' man cat ro- wo- as-\n",
      "bot man tro- but man tro-\n",
      "naut naut\n",
      "\n",
      "Figure 8: Perceptual similarity and image hash scores for\n",
      "images containing different subjects, in controlled dataset.\n",
      "\n",
      "suggesting a strong alignment with their respective prompts.\n",
      "This observation prompts an inquiry into the nature of the\n",
      "subjects’ representations: are they inherently distinct, con-\n",
      "sistently depicted by the models, or is it that humans are\n",
      "inherently better at inferring prompts related to these sub-\n",
      "jects? Inherent distinctness would imply that robots and cats\n",
      "possess unique, identiﬁable features that are readily recog-\n",
      "nized by the B32 and L14 models. Consistent depiction,\n",
      "on the other hand, would result in uniformity in represent-\n",
      "ing these subjects across various image generations. Mean-\n",
      "while, if humans are inherently better at inferring prompts\n",
      "for these particular subjects, this could also contribute to the\n",
      "observed accuracy. While cats may not always meet the de-\n",
      "sired benchmarks, in the top 25% of cases, their inference\n",
      "performance reaches the success threshold, with 66 infer-\n",
      "ence instances above 6332 and 61 instances above (?1_,4. Ex-\n",
      "cept for man and woman, all subjects exhibit at least 30 in-\n",
      "stances surpassing the thresholds.\n",
      "\n",
      "Perceptual similarity and image hash scores, as depicted\n",
      "in Figure 8, indicate a consistent representation across sub-\n",
      "jects, although cats show slightly higher scores, hinting at a\n",
      "less consistent inference process. For the image hash met-\n",
      "ric, only 30 to 40 instances for each subject show scores\n",
      "below the success threshold, indicating that lower scores,\n",
      "which signify better accuracy, are not frequently achieved.\n",
      "\n",
      " \n",
      "\n",
      "Ill\n",
      "\n",
      "robot woman astronaut\n",
      "\n",
      "70\n",
      "60\n",
      "50\n",
      "\n",
      "av\n",
      "\n",
      "E\n",
      "5 40\n",
      "\n",
      "u\n",
      "30\n",
      "20\n",
      "10\n",
      "0\n",
      "\n",
      "man cat\n",
      "\n",
      "— Not at All Similar — somewhat Similar\n",
      "— slightly similar — very similar\n",
      "\n",
      "Figure 9: Part IV similarity rating distributions for images\n",
      "containing the ﬁve different subjects.\n",
      "\n",
      "E“\n",
      "u-\n",
      "\n",
      "E“\n",
      "o\n",
      "\n",
      "        \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "E\"\n",
      "u-\n",
      "\n",
      "   \n",
      "\n",
      "     \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "E\"\n",
      "o\n",
      "\n",
      "     \n",
      "  \n",
      "\n",
      "      \n",
      "\n",
      "   \n",
      "   \n",
      "\n",
      "   \n",
      "    \n",
      "\n",
      "      \n",
      "\n",
      "2'‘\n",
      "\n",
      "UI\n",
      "cool colors\n",
      "Rustic style\n",
      "\n",
      " \n",
      "  \n",
      "    \n",
      "\n",
      "unreal render\n",
      "aright colors\n",
      "Moody lighting\n",
      "Gothic style\n",
      "Cloudy Melancholic\n",
      "\n",
      "   \n",
      "\n",
      "Arrican Savannah\n",
      "octane render\n",
      "cinematic lighting\n",
      "Futurist style\n",
      "cartoon style\n",
      "Mystical style\n",
      "\n",
      "     \n",
      "\n",
      "      \n",
      "\n",
      "   \n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "weighted Average rtating\n",
      "P 2'‘\n",
      "III a\n",
      "Asymmetry\n",
      "Pop Art style\n",
      "melancholic\n",
      "Dark nramatic\n",
      "Futurist\n",
      "Arctic Tundra\n",
      "Mystical\n",
      "\n",
      "P\n",
      "o\n",
      "\n",
      "Modifier\n",
      "\n",
      "Figure 10: Top 10 and bottom 10 modiﬁers by weighted av-\n",
      "erage ratings of the overall image similarity, by participants\n",
      "in Part IV.\n",
      "\n",
      "However, the performance in terms of perceptual similarity\n",
      "is notably poorer across all subjects, with fewer than 12 in-\n",
      "stances falling below the success threshold.\n",
      "\n",
      "According to surveyed similarity ratings (Figure 9), im-\n",
      "ages of can and axtrorlauts predominantly received “Very\n",
      "Similar” ratings (67 and 61, respectively), denoting high in-\n",
      "ference accuracy. Robots also achieved several “Very Sim-\n",
      "ilar” ratings (57) but with a noticeable amount of “Slightly\n",
      "Similar” ratings (45) as well, indicating some variability. Im-\n",
      "ages depicting man are mostly rated as “Somewhat Similar,”\n",
      "(70) which suggests moderate inference accuracy, with a dis-\n",
      "tribution of ratings across different similarity levels, similar\n",
      "to images of woman.\n",
      "\n",
      "Despite certain subjects such as cars and robots align-\n",
      "ing moderately well in certain metrics, the overall analy-\n",
      "sis across subjects underscores that human prompt inference\n",
      "generally does not meet the success thresholds, emphasizing\n",
      "the inherent challenge of the task. These results suggest that\n",
      "while there are instances of alignment, the overall capability\n",
      "of humans to accurately infer prompts remains below par.\n",
      "\n",
      "Modiﬁers. We next explored how modiﬁers present in\n",
      "the image generation prompts could affect the participants’\n",
      "prompt inference accuracy. To better quantify this, we calcu-\n",
      "\n",
      "\n",
      "late a Weighted Average Rating to assess the ranking of the\n",
      "modiﬁers through the images shown in Part IV. This metric\n",
      "is determined by assigning a normalized value to each fa-\n",
      "miliarity rating, where 1 represents “Not at All Similar” and\n",
      "4 represents “Very Similar”. The process involves counting\n",
      "the frequency of each rating, multiplying it by its normalized\n",
      "value, and then calculating the average of these products.\n",
      "Formally, the Weighted Average Rating, W’, is expressed as:\n",
      "\n",
      "H, :  fz'1/‘i\n",
      "\n",
      "21:1 f.\n",
      "\n",
      "where f; is the frequency of the i-th rating, v, is the nor-\n",
      "malized value of the i-th rating, and n is the number of rating\n",
      "types. Figure 10 shows 20 modiﬁers from our list of 121, the\n",
      "top 10 modiﬁers with the highest weighted average, in blue;\n",
      "and the bottom 10 modiﬁers, with the lowest weighted av-\n",
      "erage, in black.Notably, some of the top modiﬁers such as\n",
      "“Unreal render,” and “Cloudy Melancholic” are potentially\n",
      "challenging to be inferred directly by participants. This ob-\n",
      "servation suggests two possibilities: either the top modiﬁers\n",
      "exert substantial inﬂuence, making them easily identiﬁable\n",
      "and thus straightforward to infer by participants, or, despite\n",
      "their complexity, these modiﬁers do not signiﬁcantly impact\n",
      "the image generation process, thereby not detrimentally af-\n",
      "fecting the participants’ ability to recognize the intended im-\n",
      "\n",
      "agery.\n",
      "\n",
      "Art Background vs. Other Backgrounds. Figure 24 in\n",
      "Appendix F shows that participants with an art background\n",
      "tend to introduce more variability and slightly lower scores\n",
      "in semantic similarity and CLIP scores across both con-\n",
      "trolled (where the median of semantic similarity from B32 is\n",
      "0.465% lower on average and median CLIP score of B32 is\n",
      "0.172% lower on average) and uncontrolled (where the me-\n",
      "dian of semantic similarity from B32 is 0.292% lower on av-\n",
      "erage, CLIP score B32 is 0.057% lower on average) datasets.\n",
      "This suggests that their reﬁned ability to discern certain im-\n",
      "age characteristics may lead to more critical or complex in-\n",
      "terpretations of the AI-generated images, potentially diverg-\n",
      "ing from the target images more than those without an art\n",
      "background.\n",
      "\n",
      "Participants for both groups have their CLIP score below\n",
      "the success thresholds (green and red dashed lines in Fig-\n",
      "ure 24 indicate the thresholds 6332 and 9L1,/. respectively). In\n",
      "the controlled data set, only 1 out of 134 participants in the\n",
      "Non-Art were able to have an average CLIP B32 score above\n",
      "the threshold; while the inverse is true for the uncontrolled\n",
      "dataset; where, notably, there were only 1 participant with\n",
      "Art background (out of 36) and 13 Non-Art participants (out\n",
      "of 134) were able to achieved scores above the thresholds.\n",
      "\n",
      "Perceptual similarity and image hash scores (Figure 25)\n",
      "show negligible differences between the art and non-art\n",
      "groups (eg. perceptual similarity median only shows a\n",
      "0.02% difference between the group in the uncontrolled\n",
      "dataset, and 0.326% difference in the controlled dataset).\n",
      "Surveyed similarity ratings (Figure 23) also does not provide\n",
      "any signiﬁcant difference between two participant groups. In\n",
      "summary, the involvement of individuals with certain exper-\n",
      "tise in the ﬁeld, such as those with an art background, does\n",
      "\n",
      "not appear to signiﬁcantly enhance the accuracy of human\n",
      "prompt inference.\n",
      "\n",
      "MThrk Workers vs. University Population. Figure 26 in\n",
      "Appendix F indicates that university participants generally\n",
      "achieved higher median scores across all categories com-\n",
      "pared to MTurk workers (semantic similarity B32 is 3.168%\n",
      "higher and CLIP score B32 is 4.311% higher for uncon-\n",
      "trolled dataset on average, while the controlled dataset sees\n",
      "the semantic similarity B32 to be 2.2% higher, and CLIP\n",
      "score B32 to be be 4.446% higher on average), suggesting a\n",
      "more accurate inference of prompts. This difference is par-\n",
      "ticularly notable in the controlled dataset, where the tasks\n",
      "may be inherently more straightforward due to the structured\n",
      "nature of the prompts. However, a signiﬁcant observation\n",
      "is the broader range of scores among MTurk participants,\n",
      "pointing to a greater variability in their prompt interpreta-\n",
      "tions. In terms of success thresholds, participants for both\n",
      "groups have their CLIP scores below the success threshold.\n",
      "In the uncontrolled dataset, there were 7 participants from\n",
      "either group who achieved a CLIP B32 scores above the suc-\n",
      "cess threshold. While in the controlled dataset, there were\n",
      "only 1 out of 59 participants in the university group with\n",
      "their score above the threshold.\n",
      "\n",
      "Perceptual similarity and image hash scores (Figure 27 in\n",
      "Appendix F) further supports this observation, showing that\n",
      "median scores are closely aligned between university and\n",
      "MTurk participants. In summary, while university partici-\n",
      "pants tend to perform better on average, indicating poten-\n",
      "tially more precise prompt inferences, the variability among\n",
      "MTurk workers highlights a diverse range of interpretations\n",
      "and evaluations.\n",
      "\n",
      "AI Tool Familiarity. Participants with limited familiar-\n",
      "ity with generative AI tools exhibited higher inference ac-\n",
      "curacy across metrics, such as higher B32 and L14 CLIP\n",
      "scores (Figures 28a and 28b in Appendix F), as well as lower\n",
      "image hash and perceptual similarity scores (Figures 28c\n",
      "and 28d) in the controlled dataset. This outcome may sug-\n",
      "gest that participants without extensive AI tool exposure\n",
      "are either applying broader interpretations that align well\n",
      "with AI-generated images or that their fresh intuition-based\n",
      "prompt inference aligns better with the txt2img models’\n",
      "generative behavior. The uncontrolled dataset reafﬁrms this\n",
      "trend, where participants with minimal AI tool familiarity\n",
      "again demonstrated slightly higher prompt inference accu-\n",
      "racy (Figures 29a to 29d). Being more familiar with gener-\n",
      "ative AI tools does not seem to affect how close the partic-\n",
      "ipant’s score is to the success thresholds and overall all fa-\n",
      "miliarity categories still fall short of the success thresholds\n",
      "across all the metrics.\n",
      "\n",
      "The cumulative ﬁndings thus far indicate that human per-\n",
      "formance in prompt inference is generally below par, re-\n",
      "gardless of the image generation models, subjects or mod-\n",
      "iﬁers involved, or the demographic backgrounds of the par-\n",
      "ticipants. Yet, before drawing any deﬁnitive conclusions, we\n",
      "will explore the potential for enhancing prompt inference ac-\n",
      "curacy through a synergistic approach that combines AI and\n",
      "human capabilities.\n",
      "\n",
      "\n",
      "P\n",
      "In\n",
      "\n",
      "          \n",
      "\n",
      "0.9\n",
      "W 0' 0.x\n",
      "5 0.x 5\n",
      "g 5 0 7\n",
      "N 0.7 Q '\n",
      "E E.‘\n",
      "E M; E 0.5\n",
      "d d\n",
      "o_5 5 0.5\n",
      "0.4 8 0\n",
      "0.4 9\n",
      "Dream- Realistic SD Dream- Realistic SD\n",
      "Shaper Visinn XL Shaper Visinn XL\n",
      "XL 5 XL 5\n",
      "(3) (b)\n",
      "\n",
      "1 Human Scares 1 AlS:nres 1 Human-AIS:nres\n",
      "\n",
      "Figure 11: B32 (a) and L14 (b) CLIP scores for different\n",
      "\n",
      "     \n",
      "\n",
      "models from prompts generated by 1uman, AI, and human-\n",
      "AI combination.\n",
      "35 0.9\n",
      "2 -‘E\n",
      "3 L“\n",
      "V, 39 E 0.3\n",
      ": 25 g 0.7\n",
      ".. ..\n",
      "3 3\n",
      "E 20 D E 0.5\n",
      "3 a\n",
      "15  _ 0.5 _ _\n",
      "Dream- Reallstn: SD Dream- Reallstn: SD\n",
      "Shaper Visinn XL Shaper Visinn XL\n",
      "XL 5 XL 5\n",
      "(3) ('3)\n",
      "\n",
      "1 Human Scares 1 AlS:nres 1 Human-AIS:nres\n",
      "\n",
      "Figure 12: Image hash score (a) and perceptual similarity (b)\n",
      "for different models from prompts generated by human, AI,\n",
      "and human-AI combination.\n",
      "\n",
      "5.3 Human-AI Combined Inference\n",
      "\n",
      "We next examine the effectiveness of combining human in-\n",
      "ferences with AI inferences, particularly using CLIP inter-\n",
      "rogator for inference and then merging them using GPT-\n",
      "4 (as outlined in Section 4.3), towards accurately infer-\n",
      "ring prompts for AI-generated images. CLIP score analysis\n",
      "shows that combination of humans and AI inferred prompts\n",
      "generally results in higher image CLIP scores (Figure 11).\n",
      "Speciﬁcally, the B32 scores were found to be 3.126% higher\n",
      "on average in a combined setting compared to individual ef-\n",
      "forts. However, when compared to the images generated by\n",
      "prompts inferred by CLIP Interrogator, the B32 score saw\n",
      "a 0.23% decrease on average, due to the Realistic Vision 5-\n",
      "generated images performing worse; although Midjoumey-\n",
      "generated images performed 1.265% better, and SDXL im-\n",
      "ages with a 1.099% increase in performance (Figure 11a).\n",
      "With that being said, the overall improvement highlights\n",
      "how combining human inference with AI inference can\n",
      "lead to slightly better accuracy in matching prompts with\n",
      "\n",
      ".°\n",
      "to\n",
      "\n",
      ".°\n",
      "nu\n",
      "\n",
      ".°\n",
      "4\n",
      "\n",
      "CLIP H32 Score\n",
      ".° .°\n",
      "UI 9|\n",
      "\n",
      "CLIP L14 Score\n",
      "\n",
      "   \n",
      "\n",
      ".°\n",
      "a\n",
      "\n",
      "man cat rn- wn- anna-\n",
      "but man naut\n",
      "\n",
      "(3) (b)\n",
      "\n",
      "- Human Scares - AI Scares - Human-Al Scares\n",
      "\n",
      "man cat m- wn- anna-\n",
      "but man naut\n",
      "\n",
      "Figure 13: B32 (a) and L14 (b) CLIP scores for different\n",
      "subjects and modiﬁers from prompts generated by human,\n",
      "AI, and human-AI combination.\n",
      "\n",
      "  \n",
      "\n",
      "35 .. g9 5 ,, 30-9 0 8 ° 9\n",
      "\n",
      "E '5. °\n",
      "0 30 .= 0.a\n",
      "\n",
      "VI\n",
      "\n",
      "= -E\n",
      "\n",
      "3 — —  —  — — -3- — — _\n",
      "\n",
      ":: 25 a D H g0.7\n",
      "\n",
      ",, e cg ° on\n",
      "\n",
      "5, Q.\n",
      "\n",
      "E E D 3 O\n",
      "\n",
      "_ 20 E 9-5 5\n",
      "\n",
      "a G — ——°———~§~e ————————————— ——\n",
      "15 0.5 °\n",
      "\n",
      " \n",
      "\n",
      "man cat m- wn- astra-\n",
      "but man naut\n",
      "\n",
      "(3) (b)\n",
      "\n",
      "1 Human Scares 1 AI Scares 1 Human-AI Scares\n",
      "\n",
      "man cat m- wn- anna-\n",
      "but man naut\n",
      "\n",
      "Figure 14: Image hash score (a) and perceptual similarity\n",
      "score (b) for different subjects and modiﬁers from prompts\n",
      "generated by human, AI, and human-AI combination.\n",
      "\n",
      "generated images. The overlap in scores across different\n",
      "txt2img models suggests that while human-AI combined\n",
      "prompts enhances accuracy, the degree of improvement may\n",
      "not be drastically distinct among different AI models.\n",
      "\n",
      "Perceptual similarity and image hash score (Figure 12)\n",
      "also indicate that images generated through human-AI com-\n",
      "bined prompts align more closely with target images, as\n",
      "evident from the slightly lower average hash scores (by\n",
      "1.417%) and perceptual similarity scores (by 0.905%). Fig-\n",
      "ures 13 and 14 further explores this combined prompt in-\n",
      "ference, showcasing that human-AI combined prompts not\n",
      "only consistently improves inference across various subjects\n",
      "but also indicates a nuanced enhancement in how speciﬁc\n",
      "subjects and modiﬁers are interpreted and matched (hash\n",
      "score median is on average 1.438% lower, while perceptual\n",
      "similarity median is on average 1.431% lower).\n",
      "\n",
      "For all 4 types of metrics, combining AI inferred prompts\n",
      "through CLIP interrogator with human inferred prompt has a\n",
      "positive effect on the overall accuracy. While the human-AI\n",
      "combined prompts still did not reach the respective metrics’\n",
      "\n",
      "\n",
      "success thresholds, its performance is slightly better than\n",
      "that of purely human inferred prompts. This suggests that\n",
      "accurate prompt inference by a human would beneﬁt from\n",
      "combination with AI inference.\n",
      "\n",
      "The enhanced accuracy in prompt inference through the\n",
      "combination of human and AI efforts can be attributed to\n",
      "the complementary skills and knowledge each brings. Hu-\n",
      "mans excel in creative and contextual understanding, while\n",
      "AI provides extensive data analysis and pattern recognition\n",
      "capabilities. This combination allows for error reduction, it-\n",
      "erative reﬁnement, and a broader knowledge base, leading\n",
      "to more accurate prompts. Despite these beneﬁts, the fact\n",
      "that performance did not reach the success thresholds sug-\n",
      "gests that further optimizations in human-AI interaction are\n",
      "required.\n",
      "\n",
      "In summary, the empirical evidence gathered from the\n",
      "evaluation, shows that while humans alone may struggle\n",
      "with prompt inference, their efforts, when combined with\n",
      "AI, demonstrate a modest improvement. The ﬁndings sug-\n",
      "gest there’s room for growth in human-AI collaborations,\n",
      "which could eventually enhance the efﬁcacy of prompt re-\n",
      "construction. Until such advancements are realized, prompt\n",
      "marketplaces maintain their practicality as a business model\n",
      "in the realm of AI-generated art.\n",
      "\n",
      "Conclusion\n",
      "\n",
      "The study presented in this paper explores the intellec-\n",
      "tual property considerations of prompts in the realm of AI-\n",
      "generated art, particularly focusing on the ability of humans\n",
      "to infer these prompts by solely examining the resulting art-\n",
      "works. The below par results from our human only prompt\n",
      "inference experiments suggest that the performance of hu-\n",
      "man participants is inﬂuenced by the complexities in the\n",
      "prompt and the generated artwork. However, the combina-\n",
      "tion of prompts from Al models such as CLIP interroga-\n",
      "tor has a positive effect on improving the inference efforts\n",
      "of the human participants. Collectively, these observations\n",
      "point towards the feasibility of generative AI prompt mar-\n",
      "ketplaces as viable business models, where the uniqueness\n",
      "and creativity of prompts can be valued and traded while\n",
      "maintaining their secure intellectual property. Moreover, the\n",
      "exploration of combining human insights with AI in improv-\n",
      "ing prompt inference opens new avenues for research.\n",
      "\n",
      "References\n",
      "\n",
      "Brade, S.; Wang, B.; Sousa, M.; Oore, S.; and Grossman,\n",
      "T. 2023. Promptify: Text-to-image generation through inter-\n",
      "active prompt exploration with large language models. In\n",
      "ACM UIST.\n",
      "\n",
      "CLIP Interrogator. (web). https://huggingface.co/spaces/\n",
      "pharmapsychotic/CLIP-Interrogator.\n",
      "\n",
      "Hugging Face. (web)a. runwayml/stable-diffusion-V1-5.\n",
      "https://huggingface.co/runwaym1/stab1e-diffusion-V1-5.\n",
      "\n",
      "Hugging Face. (web)b. SG161222/Rea1istic-Vision-\n",
      "V5.1-noVAE. https://huggingface.co/SG161222/Rea1istic-\n",
      "Vision-V5.1-noVAE.\n",
      "\n",
      "Hugging Face. (web)c. stabilityai/stable-diffusion-xl-base-\n",
      "1.0. https://huggingface.co/stabi1ityai/stab1e-diffusion-xl-\n",
      "base- 1.0.\n",
      "\n",
      "Lanz, J. A. (web). Inside the Lucrative New Business of\n",
      "Selling AI Prompts. https://decrypt.co/137689/lucrative-\n",
      "new-business-selling-ai-prompts.\n",
      "\n",
      "Li, J.; Li, D.; Xiong, C.; and Hoi, S. 2022. Blip: Boot-\n",
      "strapping language-image pre-training for uniﬁed vision-\n",
      "language understanding and generation. In ICML.\n",
      "\n",
      "Lyu, Y.; Wang, X.; Lin, R.; and Wu, J. 2022. Communica-\n",
      "tion in human—AI co-creation: Perceptual analysis of paint-\n",
      "ings generated by text-to-image system. Applied Sciences,\n",
      "12(22).\n",
      "\n",
      "Midlourney. (web)a. methexis-inc / img2prompt. https://\n",
      "replicate.com/methexis-inc/img2prompt.\n",
      "\n",
      "Midlourney. (web)b. Midjourney Model Versions. https:\n",
      "//docs.midjourney.com/docs/model-versions.\n",
      "\n",
      "Nichol, A. Q.; Dhariwal, P; Ramesh, A.; Shyam, P;\n",
      "Mishkin, P.; Mcgrew, B.; Sutskever, I.; and Chen, M. 2022.\n",
      "GLIDE: Towards Photorealistic Image Generation and Edit-\n",
      "ing with Text-Guided Diffusion Models. In ICML.\n",
      "\n",
      "OpenAI, R. 2023. GPT-4 technical report. ArXiv, 2303.\n",
      "Podell, D.; English, Z.; Lacey, K.; Blattmann, A.; Dockhorn,\n",
      "T.; Muller, J.; Penna, J.; and Rombach, R. 2023. SDXL:\n",
      "Improving latent diffusion models for high-resolution image\n",
      "synthesis. ArXiv.\n",
      "\n",
      "PromptBase. (web). https://promptbase.com/tandcs.\n",
      "\n",
      "Promptrr.io. (web). Terms of Service — Promptrr.io. https:\n",
      "//promptrr.io/terms-of-service/.\n",
      "\n",
      "Ramesh, A.; Pavlov, M.; Goh, G.; Gray, S.; Voss, C.; Rad-\n",
      "ford, A.; Chen, M.; and Sutskever, I. 2021. Zero-shot text-\n",
      "to-image generation. In ICML.\n",
      "\n",
      "Reimers, N.; and Gurevych, I. 2019. Sentence-BERT: Sen-\n",
      "tence embeddings using siamese bert-networks. ArXiv.\n",
      "Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Om-\n",
      "mer, B. 2022. High-resolution image synthesis with latent\n",
      "diffusion models. In IEEE/C VF CVPR.\n",
      "\n",
      "Shen, X.; Qu, Y.; Backes, M.; and Zhang, Y 2023. Prompt\n",
      "Stealing Attacks Against Text-to-Image Generation Models.\n",
      "ArXiv.\n",
      "\n",
      "Struppek, L.; Hintersdorf, D.; and Kersting, K. 2022. Rick-\n",
      "rolling the artist: Injecting invisible backdoors into text-\n",
      "guided image generation models. ArXiv.\n",
      "\n",
      "Timothy, M. (web). Are Premium AI Prompts Worth the\n",
      "Money? https://www.makeuseof.com/should-you-buy-ai-\n",
      "prompts/.\n",
      "\n",
      "Wang, 1; Chan, K. C.; and Loy, C. C. 2023. Exploring clip\n",
      "for assessing the look and feel of images. In AAAI Confer-\n",
      "ence on Artiﬁcial Intelligence, volume 37.\n",
      "\n",
      "Wang, J.; Liu, Z.; Zhao, L.; Wu, Z.; Ma, C.; Yu, S.; Dai,\n",
      "H.; Yang, Q.; Liu, Y; Zhang, S.; et a1. 2023. Review of\n",
      "large vision models and visual prompt engineering. Meta-\n",
      "Radiolngy, 100047.\n",
      "\n",
      "Wu, Y.; Yu, N.; Li, Z.; Backes, M.; and Zhang, Y. 2022.\n",
      "Membership inference attacks against text-to-image genera-\n",
      "tion models. ArXiv.\n",
      "\n",
      "\n",
      "Wulfken, W. (web). MidJourney Styles and Keywords\n",
      "Reference. https://github.com/willwulfken/MidJourney-\n",
      "Styles- and- Keywords-Reference/tree/main/Pages/MJ _V4/\n",
      "Style.Pages/Just_The_Style.\n",
      "\n",
      "Zhai, S.; Dong, Y; Shen, Q.; Pu, S.; Fang, Y.; and Su, H.\n",
      "2023. Text-to-image diffusion models can be easily back-\n",
      "doored through multimodal data poisoning. ArXiv.\n",
      "\n",
      "Zhang, R.; Isola, P.; Efros, A. A.; Shechtman, E.; and Wang,\n",
      "0. 2018. The Unreasonable Effectiveness of Deep Features\n",
      "as a Perceptual Metric. In IEEE/C VF CVPR.\n",
      "\n",
      "Paper Checklist\n",
      "\n",
      "1. For most authors...\n",
      "\n",
      "(a) Would answering this research question advance sci-\n",
      "ence without violating social contracts, such as violat-\n",
      "ing privacy norms, perpetuating unfair proﬁling, exac-\n",
      "erbating the socio-economic divide, or implying disre-\n",
      "spect to societies or cultures? Yes, this survey should\n",
      "not have any of the negative impact mentioned above.\n",
      "\n",
      "(b) Do your main claims in the abstract and introduction\n",
      "accurately reﬂect the paper’s contributions and scope?\n",
      "Yes\n",
      "\n",
      "(c) Do you clarify how the proposed methodological ap-\n",
      "proach is appropriate for the claims made? NA\n",
      "\n",
      "(d) Do you clarify what are possible artifacts in the data\n",
      "used, given population-speciﬁc distributions? .\\'A\n",
      "\n",
      "(e) Did you describe the limitations of your work? NA\n",
      "\n",
      "(0 Did you discuss any potential negative societal im-\n",
      "pacts of your work? No\n",
      "\n",
      "(g) Did you discuss any potential misuse of your work?\n",
      ".\\'A\n",
      "\n",
      "(h) Did you describe steps taken to prevent or mitigate po-\n",
      "tential negative outcomes of the research, such as data\n",
      "and model documentation, data anonymization, re-\n",
      "sponsible release, access control, and the reproducibil-\n",
      "ity of ﬁndings‘? NA\n",
      "\n",
      "(i) Have you read the ethics review guidelines and en-\n",
      "sured that your paper conforms to them? Yes\n",
      "\n",
      "2. Additionally, if your study involves hypotheses testing...\n",
      "\n",
      "(a) Did you clearly state the assumptions underlying all\n",
      "theoretical results? I\\'.-\\\n",
      "\n",
      "(b) Have you provided justiﬁcations for all theoretical re-\n",
      "sults? .\\'A\n",
      "\n",
      "(c) Did you discuss competing hypotheses or theories that\n",
      "might challenge or complement your theoretical re-\n",
      "sults? .\\'A\n",
      "\n",
      "(d) Have you considered altemative mechanisms or expla-\n",
      "nations that might account for the same outcomes ob-\n",
      "served in your study? .\\'A\n",
      "\n",
      "(e) Did you address potential biases or limitations in your\n",
      "theoretical framework? NA\n",
      "\n",
      "(0 Have you related your theoretical results to the existing\n",
      "literature in social science? NA\n",
      "\n",
      "(g) Did you discuss the implications of your theoretical\n",
      "results for policy, practice, or further research in the\n",
      "social science domain? NA\n",
      "\n",
      "3. Additionally, if you are including theoretical proofs...\n",
      "\n",
      "(a) Did you state the full set of assumptions of all theoret-\n",
      "ical results? NA\n",
      "\n",
      "(b) Did you include complete proofs of all theoretical re-\n",
      "sults? NA\n",
      "\n",
      "4. Additionally, if you ran machine learning experiments...\n",
      "\n",
      "(a) Did you include the code, data, and instructions\n",
      "needed to reproduce the main experimental results (ei-\n",
      "ther in the supplemental material or as a URL)? NA\n",
      "\n",
      "(b) Did you specify all the training details (e.g., data splits,\n",
      "hyperparameters, how they were chosen)? NA\n",
      "\n",
      "(c) Did you report error bars (e.g., with respect to the ran-\n",
      "\n",
      "dom seed after running experiments multiple times)?\n",
      "NA\n",
      "\n",
      "(d) Did you include the total amount of compute and the\n",
      "type of resources used (e.g., type of GPUs, internal\n",
      "cluster, or cloud provider)? I\\'.-\\\n",
      "\n",
      "(e) Do you justify how the proposed evaluation is sufﬁ-\n",
      "cient and appropriate to the claims made? NA\n",
      "\n",
      "(f) Do you discuss what is “the cost“ of misclassiﬁcation\n",
      "and fault (in)tolerance? l\\'A\n",
      "\n",
      "5. Additionally, if you are using existing assets (e.g., code,\n",
      "data, models) or curating/releasing new assets, without\n",
      "compromising anonymity...\n",
      "\n",
      "(a) If your work uses existing assets, did you cite the cre-\n",
      "ators? NA\n",
      "\n",
      "(b) Did you mention the license of the assets? .\\'A\n",
      "\n",
      "(c) Did you include any new assets in the supplemental\n",
      "material or as a URL? NA\n",
      "\n",
      "(d) Did you discuss whether and how consent was ob-\n",
      "tained from people whose data you’re using/curating?\n",
      "NA\n",
      "\n",
      "(e) Did you discuss whether the data you are using/cu-\n",
      "rating contains personally identiﬁable information or\n",
      "offensive content? I\\'.-\\\n",
      "\n",
      "(f) If you are curating or releasing new datasets, did you\n",
      "discuss how you intend to make your datasets FAIR?\n",
      "NA\n",
      "\n",
      "(g) If you are curating or releasing new datasets, did you\n",
      "create a Datasheet for the Dataset? NA\n",
      "\n",
      "6. Additionally, if you used crowdsourcing or conducted\n",
      "research with human subjects, without compromising\n",
      "anonymity...\n",
      "\n",
      "(a) Did you include the full text of instructions given\n",
      "to participants and screenshots? Yes, details the Ap-\n",
      "pendix.\n",
      "\n",
      "(b) Did you describe any potential participant risks, with\n",
      "mentions of Institutional Review Board (IRB) ap-\n",
      "\n",
      "provals? No, the study was granted an Exempt status,\n",
      "and posed no risks to participants.\n",
      "\n",
      "\n",
      "c i ouincue eesimae our wae ai o\n",
      "\n",
      ")Ddy ldth t tdh ly gpdt\n",
      "participants and the total amount spent on participant\n",
      "compensation? .\\'A\n",
      "\n",
      "(d) Did you discuss how data is stored, shared, and dei-\n",
      "dentiﬁed? .\\'A\n",
      "\n",
      "A Prompts and Marketplaces\n",
      "\n",
      "A.1 Prompt Marketplaces\n",
      "\n",
      "The integration of txt2img models in digital art creation\n",
      "has led to the development of Al prompt marketplaces, such\n",
      "as PromptHero\", Promptrr.io7, Prompti Al“, PromptBase9,\n",
      "and CiviLAl'°, These platforms facilitate the buying, sell-\n",
      "ing, or sharing of prompts designed for various txt2img\n",
      "models (Figure 15). Prompt marketplaces operate on a busi-\n",
      "ness model in which users can create and submit their\n",
      "own prompts or purchase those created by others. This has\n",
      "turned prompt creation into a proﬁtable activity, as effec-\n",
      "tive prompts greatly enhance the quality of Al-generated\n",
      "art (Lanz (web). Users selling prompts on these platforms\n",
      "can earn income, while buyers gain access to a diverse range\n",
      "of ready-to-use prompts, enhancing their productivity and\n",
      "creativity in Al art generation. These marketplaces typically\n",
      "use a commission-based revenue model, where prompt au-\n",
      "thors pay the platform a part of their revenue whenever the\n",
      "platform sells their prompt to a customer (Timothy (web),\n",
      "These platforms also generally treat prompts as intellectual\n",
      "property (PromptBase; Promptrr.io), recognizing and pro-\n",
      "tecting the creative effort involved in their creation.\n",
      "\n",
      "\\\n",
      "\n",
      " \n",
      "\n",
      "Figure 15: A PromptBase listing selling the prompt to a spe-\n",
      "ciﬁc style of artwork,\n",
      "\n",
      "A.2 Subjects and Modiﬁers\n",
      "\n",
      "The use of subjects and modifiers in prompts plays an im-\n",
      "portant role in guiding a txt2img model to produce im-\n",
      "ages that align with the creator’s vision. The subject serves\n",
      "\n",
      "ﬁhttpsz//promptherucom\n",
      "7https://promptrrio\n",
      "xhttpsz//promptixai\n",
      "ghttpsz//promptbasecom\n",
      "mhttps://civitai.com\n",
      "\n",
      "as the core theme or focus of the image, while modiﬁers pro-\n",
      "vide the speciﬁcations that reﬁne and shape the ﬁnal output.\n",
      "Subjects can range from concrete objects, like “cat,” “forest”\n",
      "or “robot,” to more abstract concepts, such as “solitude” or\n",
      "“chaos.” On the other hand, the modiﬁers adjust the images’s\n",
      "aesthetic by specifying attributes such as color, texture, time\n",
      "of day, or emotional ambiance, effectively guiding the Al to-\n",
      "wards a more targeted and reﬁned artistic rendition. Figure 1\n",
      "demonstrates an example of how adding different modiﬁers\n",
      "alongside a subject (cat) produces varied image generations.\n",
      "Prompts for txt2img models are typically limited by 300\n",
      "to 400 input characters (depending on the model) on how\n",
      "many subjects and modiﬁers can be speciﬁed for each im-\n",
      "age generation,\n",
      "\n",
      "B Analyzed txt2img Model Details\n",
      "\n",
      "- MidJoumey V5.0 (Midjourney (web) from Midjourney\n",
      "Inc. is a closed-source txt2img model which is recog-\n",
      "nized for generating images that often exhibit a unique\n",
      "artistic and abstract quality. Due to its closed-source na-\n",
      "ture, not much is known about the architecture and param-\n",
      "eters of its image generator model or the dataset used to\n",
      "train it. Midloumey v5,0 can natively generate images of\n",
      "1024>< 1024 pixels, using a Discord bot with /imagine\n",
      "[Prompt] command.\n",
      "\n",
      "- Stable Diffusion XL (SDXL) (Podell et al, 2023; Rom-\n",
      "bach et al. 2022), created by Stability Al, employs a vari-\n",
      "ational autoencoder (VAE) in conjunction with a cross-\n",
      "attention transforrner-based architecture as the generator.\n",
      "Stable Diffusion XL can also natively generate images of\n",
      "1024>< 1024 pixels, using direct interface with the publicly\n",
      "available model (Hugging Face c).\n",
      "\n",
      "- DreamShaper XL was trained and ﬁne-tuned over Stable\n",
      "Diffusion XL (Podell et al, 2023), with a focus on generat-\n",
      "ing photo-realistic fantasy images. Similar to Stable Dif-\n",
      "fusion XL, DreamShaper XL can also natively generate\n",
      "images of l024>< 1024 pixels.\n",
      "\n",
      "- Realistic Vision VS (Hugging Face b) was trained and\n",
      "ﬁne-tuned over Stable Diffusion 1.5 (Hugging Face a),\n",
      "with a focus on generating photo-realistic images. How-\n",
      "ever, because Stable Diffusion 1.5 was trained using\n",
      "512><512 images, Realistic Vision 5 output is also lim-\n",
      "ited to 5 l2><5 12 pixels, Generating images at a resolution\n",
      "of 1024>< 1024 pixels with a model originally trained for\n",
      "512><512 pixels often leads to the emergence of undesir-\n",
      "able artifacts, as the diffusion models tend to replicate the\n",
      "quantity of subjects, But for an equitable comparison with\n",
      "other images in our study, we upscaled Realistic Vision V5\n",
      "images with Highresﬁx and ScuNET-PSNR upscaler to\n",
      "1024><1024 pixels, Highresﬁx is a generation technique\n",
      "where the ﬁrst half of sampling iterations are done at na-\n",
      "tive resolution, and then the later half at the targeted up-\n",
      "scaled resolution. This can add more accurate details to\n",
      "the upscaled images while the generation prompt is still in\n",
      "context, without the aforementioned artifacts being intro-\n",
      "duced.\n",
      "\n",
      "\n",
      "C Pre-Survey Details and Results\n",
      "\n",
      "Age:\n",
      "18-24\n",
      "25-34\n",
      "35-44\n",
      "45-54\n",
      "55-64\n",
      "_ 65-74\n",
      "75+\n",
      "\n",
      "Gender:\n",
      "\n",
      "Male\n",
      "Female\n",
      "Other\n",
      "\n",
      "[For University Participants] Major/Department:\n",
      "\n",
      "[For M'l\\1rk Participants] Occupation:\n",
      "\n",
      "Professional/Managerial\n",
      "Skilled Trades\n",
      "\n",
      "Service Industry\n",
      "\n",
      "Sales and Marketing\n",
      "Healthcare and Medical\n",
      "\n",
      "Arts and Entertainment\n",
      "Education and Academia\n",
      "Manufacturing and Production\n",
      "Other\n",
      "\n",
      "Country:\n",
      "\n",
      "Familiarity with AI generation tools:\n",
      "\n",
      "Text Generation Tools:\n",
      "\n",
      "Not at All Familiar\n",
      "Slightly Familiar\n",
      "Somewhat Familiar\n",
      "Very Familiar\n",
      "\n",
      "Image Generation Tools:\n",
      "\n",
      "Not at All Familiar\n",
      "Slightly Familiar\n",
      "Somewhat Familiar\n",
      "Very Familiar\n",
      "\n",
      "Audio Generation Tools:\n",
      "\n",
      "Not at All Familiar\n",
      "Slightly Familiar\n",
      "Somewhat Familiar\n",
      "Very Familiar\n",
      "\n",
      "Video Generation Tools:\n",
      "\n",
      "Not at All Familiar\n",
      "Slightly Familiar\n",
      "Somewhat Familiar\n",
      "Very Familiar\n",
      "\n",
      "Figure 16: Demographic survey form.\n",
      "\n",
      ",? 2.2%\n",
      "A4 0.4%\n",
      "\n",
      " \n",
      "\n",
      "59.1%\n",
      "\n",
      "1 :5-44 1 45-54 1 55-74\n",
      "1 25-34 1 55-54 1 75+\n",
      "1 15-24 Male 1 Female 1 other\n",
      "\n",
      "(a) Age distribution. (b) Gender distribution.\n",
      "\n",
      "Figure 17: Demographic distribution of survey participants.\n",
      "\n",
      "D Details of the Metrics\n",
      "\n",
      "D.1 MSQ Scores (Survey Part I)\n",
      "\n",
      "The scoring mechanism for evaluating multiple select ques-\n",
      "tions (MSQs) in Part I with one or more correct answers on\n",
      "subject and modiﬁers pertaining to each displayed image is\n",
      "formalized through the following formula. The formula aims\n",
      "to give an MSQ score from 0 to 2 for each question, and is\n",
      "commonly used by learning management systems such as\n",
      "Canvas”. This metric serves to give an initial baseline for\n",
      "human prompt inference, as it only requires user to select\n",
      "the correct subject and modiﬁers from a set of given options.\n",
      "\n",
      "I\\rIS'QS'«.m'e;=max{(),min{T,(T-1{S(:gj,,“:n;) + TOEX 1}}\n",
      "\n",
      "where:\n",
      "\n",
      "T : 2 represents the total available points for a question,\n",
      "1 point for correct subject selection, 1 point for correct\n",
      "modiﬁer(s) selection.\n",
      "\n",
      "- Sc is the number of correctly selected options by the par-\n",
      "ticipant.\n",
      "- C is the total number of correct options for a question.\n",
      "\n",
      "- I is the number of incorrect options selected by the partic-\n",
      "ipant.\n",
      "\n",
      "- 1{g(_=(] Md 1:0} is an indicator function that equals 1 if the\n",
      "participant selects all correct options without selecting any\n",
      "incorrect ones (Sn : C and I : 0), and 0 otherwise.\n",
      "\n",
      "This formula integrates the conditions for awarding full\n",
      "points and applying penalties for incorrect selections into a\n",
      "singular expression. The use of the max and min functions\n",
      "ensures that the ﬁnal score remains within the acceptable\n",
      "range of U to T : 2 points. The indicator function facilitates\n",
      "the condition under which full points are awarded, while the\n",
      "penalty for incorrect selections is universally applied but is\n",
      "effectively neutralized when full points are granted due to\n",
      "correct selection criteria being met. This scoring framework\n",
      "was devised to offer a balanced assessment of participant\n",
      "knowledge and decision-making skills, reﬂecting both the\n",
      "breadth of correct understanding and the penalties for inac-\n",
      "curacies.\n",
      "\n",
      "1Ihttps://www.insLructure.com/canvas\n",
      "\n",
      "\n",
      "D.2 Image Hash\n",
      "\n",
      "The imagehash library in Python” offers a compelling\n",
      "approach for measuring prompt inference accuracy, specif-\n",
      "ically by comparing the original AI-generated art with an\n",
      "art generated from an inferred prompt. imagehash gen-\n",
      "erates perceptual hashes of images, where images that are\n",
      "similar result in hashes that are closely aligned. The simi-\n",
      "larity between these hashes is measured using the Hamming\n",
      "distance, a measure for comparing two binary data strings.\n",
      "By calculating the Hamming distance between the hashes\n",
      "of two images, we can quantitatively assess their similar-\n",
      "ity. A lower image hash score (difference) implies the two\n",
      "images are similar, and vice versa. For example, images in\n",
      "Figure 18a and Figure 18b have an image hash score of 26,\n",
      "where they were generated from the same prompt. In con-\n",
      "trast, Figure 18b and Figure 18c have a slightly higher im-\n",
      "age hash score of 28 when generated from a slightly different\n",
      "prompt, and Figure 18b and Figure 18d have a signiﬁcantly\n",
      "higher image hash score of 34 as they were generated from\n",
      "very different prompts.\n",
      "\n",
      "D.3 Perceptual Similarity\n",
      "\n",
      "Perceptual similarity metric (Zhang et al. 2018) captures\n",
      "the degree to which two images are perceived to be simi-\n",
      "lar by human observers. This approach goes beyond tradi-\n",
      "tional pixel-based comparisons, which often fail to capture\n",
      "the nuances of human visual perception. Instead, perceptual\n",
      "similarity considers factors like texture, color distribution,\n",
      "structural elements, and contextual information, which are\n",
      "more aligned with how humans process visual information.\n",
      "This method can provide a more accurate and meaningful\n",
      "measure of similarity between the original AI-generated art\n",
      "with an art generated from an inferred prompt, particularly\n",
      "in applications where the visual impression of an image is\n",
      "more important than its exact pixel composition. Similar to\n",
      "image hash, a lower perceptual similarity implies the two\n",
      "images are similar, and vice versa. For example, images in\n",
      "Figure 18a and Figure 18b have perceptual similarity score\n",
      "of 0.526, where they were generated from the same prompt.\n",
      "In contrast, Figure 18a and Figure 18c have a slightly higher\n",
      "perceptual similarity score of 0.585 when generated from\n",
      "a slightly different prompt, and Figure 18a and Figure 18d\n",
      "\n",
      "'lhttps://pypi.org/project/ImageHash/\n",
      "\n",
      " \n",
      "\n",
      "Figure 18: SDXL generations using the same prompt “photo\n",
      "ofa huge white cat sitting on a cloud in the sky” for images\n",
      "(a) and (b), a similar but less speciﬁc prompt “cat on cloud”\n",
      "for image (c), and an entirely different prompt “a glowing\n",
      "jellyﬁsh underwater, breathtaking” for image ((1).\n",
      "\n",
      "have a signiﬁcantly higher perceptual similarity score of\n",
      "0.793 as they were generated from very different prompts.\n",
      "\n",
      "D.4 Image Embedding Similarity (CLIP Score)\n",
      "\n",
      "The CLIP score between an image and a prompt (text) mea-\n",
      "sures the cosine similarity between their embeddings (Wang,\n",
      "Chan, and Loy 2023). A higher score indicates greater rel-\n",
      "evance or similarity, as perceived by the CLIP model, be-\n",
      "tween the text and the image. This score is particularly use-\n",
      "ful in our objective to determine the accuracy or relevance of\n",
      "an image in relation to the textual input that was inferred to\n",
      "be used in its creation. We employ two state-of-the-art CLIP\n",
      "models in our evaluation, OpenAI’s ViT-L/14 Transformer”\n",
      "(L14) and ViT-B/32 Transformer” (B32). More speciﬁcally,\n",
      "we utilize SentenceTransformers (Reimers and Gurevych\n",
      "2019) wrapped L14 and B32 models for simultaneously cal-\n",
      "culating both the CLIP score and semantic similarity when\n",
      "the embeddings are generated, which is described next. Un-\n",
      "like image hash and perceptual similarity, a higher CLIP\n",
      "score implies the two images are similar, and vice versa.\n",
      "For example, images in Figure 18a and Figure 18b have a\n",
      "CLIP score of 0.970, where they were generated from the\n",
      "same prompt. In contrast, Figure 18a and Figure 18c have a\n",
      "lower CLIP score of 0923 when generated from a slightly\n",
      "different prompt, and Figure 18a and Figure 18d have a sig-\n",
      "niﬁcantly lower CLIP score of 0.629 as they were generated\n",
      "from very different prompts.\n",
      "\n",
      "D.S Text Embedding Similarity (Semantic\n",
      "Similarity)\n",
      "\n",
      "In txt2img generation, where prompts are used to guide\n",
      "the creation of visual content, semantic text similarity is\n",
      "also an important metric in evaluating how accurately an in-\n",
      "ference deduces or reconstructed the original text prompt\n",
      "from the generated images. We use SentenceTransform-\n",
      "ers (Reimers and Gurevych 2019), an extension of the BERT\n",
      "model for efﬁciently producing sentence embeddings, for\n",
      "calculating the cosine similarity between the original prompt\n",
      "and participants’ response prompts. Similar to CLIP score, a\n",
      "higher semantic similarity implies the two prompts are sim-\n",
      "ilar, and vice versa. For example, the prompts “photo of a\n",
      "huge white cat sitting on a cloud in the sky” and “cat on\n",
      "cloud” used in Figure 18 have a high semantic similarity\n",
      "of 0.745, as measured by the L14 model. In contrast, the\n",
      "prompts “photo ofa huge white cat sitting on a cloud in the\n",
      "sky” and “a glowingjellyﬁsh underwater, breathtaking” have\n",
      "a semantic similarity of only 0.374.\n",
      "\n",
      "Surveyed Similarity Rating (from Survey Part IV). As\n",
      "outlined in Section 3.6, participants in Part IV of the study\n",
      "evaluated pairs consisting of an original image and a newly\n",
      "generated image created from a prompt provided by a previ-\n",
      "ous participant. They rated these pairs on a Likert scale con-\n",
      "sisting of “Not at All Similar”, “Slightly Similar”, “Some-\n",
      "what Similar”, and “Very Similar.” This provides an addi-\n",
      "tional measurement of image similarity, and thus prompt in-\n",
      "ference success, based on human interpretations.\n",
      "\n",
      "13https:/ﬂiuggingface.co/openai/clip-vit-large-patchl4\n",
      "14https:/ﬂiuggingface.co/openai/clip-vit-base-patch32\n",
      "\n",
      "\n",
      "E Survey Details\n",
      "\n",
      "E.l Survey Part I Question Example\n",
      "\n",
      " \n",
      "\n",
      "Question 1: Select the best fitting subject:\n",
      "\n",
      "0 man\n",
      "\n",
      "0 astronaut\n",
      "\n",
      "O robot\n",
      "\n",
      "0 cat\n",
      "\n",
      "Question 2: select the best modlflers (check all that apply):\n",
      "\n",
      "0 Pop Art\n",
      "\n",
      "B High Saturation\n",
      "\n",
      "0 Australian Outback\n",
      "U Tone mapped\n",
      "\n",
      "D Vibrant colors\n",
      "\n",
      "Next\n",
      "\n",
      "Figure 19: An example of a question for Part I. The partic-\n",
      "ipa.nt must select the best ﬁtting subject and between 1 to 5\n",
      "best ﬁtting modiﬁers.\n",
      "\n",
      "E.2 Survey Part II Question Example\n",
      "\n",
      " \n",
      "\n",
      "Question: Enter the best ﬁtting prompt\n",
      "\n",
      "Enter the prompt\n",
      "\n",
      "Next\n",
      "\n",
      "Figure 20: An example of a question for Part II. The partici-\n",
      "pant must type in a prompt for the given image.\n",
      "\n",
      "E.3 Survey Part III Question Example\n",
      "\n",
      " \n",
      "\n",
      "Question: Enter the best ﬁtting prompt\n",
      "\n",
      "Enter the prompt\n",
      "\n",
      "Next\n",
      "\n",
      "Figure 21: An example of a question for Part III. The partic-\n",
      "ipant must type in a prompt for the given image.\n",
      "\n",
      "E.4 Survey Part IV Question Example\n",
      "\n",
      " \n",
      "\n",
      "How similar are the images?\n",
      "\n",
      "0 Not at All Similar\n",
      "0 Slightly similar\n",
      "\n",
      "0 Somewhat Similar\n",
      "0 Very Similar\n",
      "\n",
      "Next\n",
      "\n",
      "Figure 22: Example of a Part IV question, where participants\n",
      "rated the similarity between pairs of Al-generated images.\n",
      "\n",
      "E.5 121 Controlled Dataset Modiﬁers and\n",
      "Frequency of Use\n",
      "\n",
      "abstract style (5), acrylic (3), aerial (2), african savanna (1),\n",
      "african savannah (1), american wild west (2), ancient chi-\n",
      "nese dynasty (4), ancient mayan (1), angry (2), arctic tun-\n",
      "dra (1), art deco (1), art deco style (1), art nouveau style\n",
      "(1), asymmetry (1), australian outback (2), backlit lighting\n",
      "(3), baroque (1), baroque style (1), bokeh lighting (3), bol-\n",
      "lywood inspired (4), bright colors (1), Caribbean island (1),\n",
      "cartoon style (1), cel shading (4), charcoal (1), cinema ren-\n",
      "der (2), cinematic lighting (1), close-up (5), cloudy lighting\n",
      "(2), cloudy melancholic (1), comic book (2), comic book\n",
      "\n",
      "\n",
      "style (3), cool colors (3), dark (3), dark colors (5), dark dra-\n",
      "matic (1), digital art (4), digital camera (4), dramatic (4),\n",
      "dramatic acrylic (1), dramatic lighting (5), dreamy lighting\n",
      "(5), dusk lighting (5), dystopian style (2), elegant (2), en-\n",
      "graving (2), excited (4), flat shading (1), futurist (1), futur-\n",
      "ist style (1), golden hour lighting (3), gooch shading (2),\n",
      "gothic european castle (6), gothic style (3), gouraud shad-\n",
      "ing (2), high contrast (5), high saturation (4), high satura-\n",
      "tion sad (1), hyper real (3), japanese garden (1), joyful (1),\n",
      "juxtaposed (2), light-hearted (2), low contrast (4), low sat-\n",
      "uration (1), maasai village (2), macro lens (5), magical (3),\n",
      "mediterranean seaside (1), melancholic (1), metallic colors\n",
      "(7), metallic noir (1), middle eastern bazaar (2), minimal-\n",
      "ist (1), minimalist style (2), moody (1), moody lighting (1),\n",
      "moonlight lighting (3), moonlit (1), mysterious (1), mysti-\n",
      "cal (1), mystical style (1), natural light lighting (2), octane\n",
      "render (1), pastel colors (2), peaceful (2), pen and ink (2),\n",
      "pencil sketch (5), phong shading (3), photorealistic (3), pixel\n",
      "art (4), polynesian island (1), pop art (1), pop art style (1),\n",
      "ray traced (4), realistic (5), renaissance italy (2), renaissance\n",
      "style (2), rustic (1), rustic style (1), sad (6), serene (3), shal-\n",
      "low depth of ﬁeld (1), sketch (2), soft lighting (1), steam-\n",
      "punk style (3), studio (1), studio lighting (3), sunrise light-\n",
      "ing (3), surrealist style (2), symmetry (4), telephoto lens (3),\n",
      "tilt-shift lens (1), tone mapped (2), unreal render (1), vibrant\n",
      "colors (6), Victorian england (2), warm colors (7), watercolor\n",
      "(4), whimsical (2), wide angle (1),\n",
      "\n",
      "E.6 100 Controlled Dataset Prompts\n",
      "- astronaut, cool colors, digital art, serene\n",
      "- astronaut, art deco\n",
      "\n",
      "- astronaut, ancient chinese dynasty, high contrast, pencil\n",
      "sketch, dramatic\n",
      "\n",
      "- astronaut, realistic, gothic european castle, symmetry, rus-\n",
      "tic\n",
      "\n",
      "- astronaut, dreamy lighting, surrealist style, peaceful,\n",
      "metallic colors\n",
      "\n",
      "- astronaut, dusk lighting, minimalist style, close-up, an-\n",
      "cient chinese dynasty\n",
      "\n",
      "- astronaut, comic book style, symmetry, hyper real, sunrise\n",
      "lighting\n",
      "- astronaut, steampunk style, angry\n",
      "- astronaut, phong shading, dark colors\n",
      "\n",
      "- astronaut, dramatic lighting, renaissance style, warm col-\n",
      "ors, pen and ink\n",
      "\n",
      "- astronaut, excited, phong shading, high saturation, ab-\n",
      "stract style\n",
      "\n",
      "- astronaut, abstract style, symmetry, high saturation, pho-\n",
      "torealistic, arctic tundra\n",
      "\n",
      "- astronaut, gothic style, telephoto lens, dusk lighting\n",
      "\n",
      "- astronaut, watercolor, elegant, cool colors, natural light\n",
      "lighting\n",
      "\n",
      "- astronaut, low contrast, ray traced, backlit lighting, close-\n",
      "“P\n",
      "\n",
      "- astronaut, vibrant colors, sunrise lighting, ancient chinese\n",
      "dynasty\n",
      "\n",
      "astronaut, bollywood inspired, dusk lighting, metallic col-\n",
      "ors\n",
      "\n",
      "astronaut, gothic style, dark, high contrast, studio lighting\n",
      "astronaut, cinematic lighting, watercolor\n",
      "\n",
      "cat, dreamy lighting\n",
      "\n",
      "cat, dark dramatic, futurist\n",
      "\n",
      "cat, cel shading, australian outback\n",
      "\n",
      "cat, metallic noir, realistic, gothic european castle\n",
      "\n",
      "cat, pixel art, dark colors\n",
      "\n",
      "cat, dramatic lighting\n",
      "\n",
      "cat, digital camera, melancholic, moonlight lighting,\n",
      "comic book style, ray traced\n",
      "\n",
      "cat, metallic colors, golden hour lighting\n",
      "\n",
      "cat, moody lighting, minimalist style, acrylic, macro lens,\n",
      "cel shading\n",
      "\n",
      "cat, digital camera, serene, macro lens, cool colors, african\n",
      "savannah\n",
      "\n",
      "cat, ancient chinese dynasty, bokeh lighting, gooch shad-\n",
      "ing, elegant\n",
      "\n",
      "cat, charcoal, tone mapped, australian outback, light-\n",
      "hearted, art deco style\n",
      "\n",
      "cat, dreamy lighting\n",
      "\n",
      "cat, pencil sketch, middle eastern bazaar, vibrant colors\n",
      "cat, dystopian style\n",
      "\n",
      "cat, magical, Vibrant colors\n",
      "\n",
      "cat, warm colors, dramatic lighting\n",
      "\n",
      "cat, warm colors, tone mapped, engraving\n",
      "\n",
      "cat, octane render, soft lighting\n",
      "\n",
      "cat, telephoto lens, abstract style\n",
      "\n",
      "man, baroque, comic book, minimalist\n",
      "\n",
      "man, ﬂat shading, maasai village, peaceful\n",
      "\n",
      "man, bollywood inspired, acrylic, comic book, dark\n",
      "\n",
      "man, dramatic, pastel colors, gothic european castle, pho-\n",
      "torealistic\n",
      "\n",
      "man, realistic, dramatic lighting, excited\n",
      "\n",
      "man, steampunk style, cinema render, digital art, metallic\n",
      "colors\n",
      "\n",
      "man, digital camera, excited, renaissance italy\n",
      "man, Vibrant colors\n",
      "\n",
      "man, metallic colors, whimsical, digital camera, golden\n",
      "hour lighting\n",
      "\n",
      "man, serene\n",
      "man, studio lighting, american wild west, unreal render\n",
      "\n",
      "man, cartoon style, gouraud shading, natural light lighting,\n",
      "dark colors, digital art\n",
      "\n",
      "man, ray traced\n",
      "man, pop art style, phong shading\n",
      "\n",
      "man, magical, watercolor, bokeh lighting, middle eastern\n",
      "bazaar\n",
      "\n",
      "man, macro lens, high saturation sad\n",
      "\n",
      "\n",
      "man, dark colors, caribbean island, dark, bokeh lighting,\n",
      "aerial\n",
      "\n",
      "man, sunrise lighting\n",
      "\n",
      "man, warm colors\n",
      "\n",
      "man, dramatic, cinema render, warm colors, backlit light-\n",
      "lng\n",
      "\n",
      "robot, american wild west, dramatic, metallic colors\n",
      "robot, gothic style\n",
      "\n",
      "robot, moody, Victorian england, sad\n",
      "\n",
      "robot, pop art, high saturation\n",
      "\n",
      "robot, high contrast, angry\n",
      "\n",
      "robot, Vibrant colors\n",
      "\n",
      "robot, low contrast, realistic\n",
      "\n",
      "robot, cel shading\n",
      "\n",
      "robot, symmetry, war1n colors, ray traced, sad, dusk light-\n",
      "lng\n",
      "\n",
      "robot, low contrast, photorealistic, magical, art nouveau\n",
      "style, juxtaposed\n",
      "\n",
      "robot, studio lighting, surrealist style\n",
      "\n",
      "robot, dystopian style\n",
      "\n",
      "robot, dark colors\n",
      "\n",
      "robot, dreamy lighting, pixel art, close-up, bollywood in-\n",
      "spired, baroque style\n",
      "\n",
      "robot, asymmetry, mystical style, cloudy lighting\n",
      "\n",
      "robot, watercolor, futurist style, maasai village, golden\n",
      "hour lighting, whimsical\n",
      "\n",
      "robot, sad, pixel art\n",
      "\n",
      "robot, sad, low contrast, gothic european castle, sketch\n",
      "robot, renaissance italy\n",
      "\n",
      "robot, wide angle, vibrant colors, cel shading, sad\n",
      "\n",
      "robot, close-up, pastel colors, ancient mayan, abstract\n",
      "style\n",
      "\n",
      "woman, hyper real, pencil sketch, aerial\n",
      "\n",
      "woman, high contrast, pencil sketch, realistic, japanese\n",
      "garden\n",
      "\n",
      "woman, Victorian england, juxtaposed, cloudy melan-\n",
      "cholic\n",
      "\n",
      "woman, light-hearted, mystical\n",
      "\n",
      "woman, macro lens, studio, moonlit, hyper real, gothic eu-\n",
      "ropean castle\n",
      "\n",
      "woman, rustic style, acrylic, cloudy lighting, bright colors\n",
      "woman, dramatic lighting, telephoto lens\n",
      "\n",
      "woman, steampunk style, high saturation, close-up,\n",
      "mediterranean seaside\n",
      "\n",
      "woman, renaissance style, moonlight lighting, wa.rm col-\n",
      "ors, dramatic acrylic\n",
      "\n",
      "woman, engraving, backlit lighting, joyful\n",
      "woman, dusk lighting, pixel art, low saturation\n",
      "\n",
      "woman, gooch shading, moonlight lighting, tilt-shift lens,\n",
      "abstract style, mysterious\n",
      "\n",
      "woman, bollywood inspired, comic book style\n",
      "\n",
      "woman, polynesian island\n",
      "\n",
      "- woman, shallow depth of ﬁeld, pen and ink, sad, metallic\n",
      "colors\n",
      "\n",
      "- woman, pencil sketch, gouraud shading\n",
      "- woman, excited, african savanna, macro lens\n",
      "\n",
      "- woman, gothic european castle, dreamy lighting, digital\n",
      "art\n",
      "\n",
      "- woman, sketch\n",
      "\n",
      "- woman, high contrast\n",
      "\n",
      "F Additional Results\n",
      "\n",
      "$ $ $\n",
      ".4 in in\n",
      "no ml in\n",
      "N a in\n",
      "N <r in\n",
      "\n",
      "22 54%\n",
      "39.41%\n",
      "31 31%\n",
      "\n",
      " \n",
      "\n",
      "u 20 no so an ion\n",
      "Percentage\n",
      "— Not at All Similar — Somewhat Similar\n",
      "\n",
      "— Slightly similar — Very similar\n",
      "\n",
      "Figure 23: Part IV similarity rating percentage from partici-\n",
      "pants with and without art background.\n",
      "\n",
      "P\n",
      "m\n",
      "\n",
      " \n",
      "\n",
      "Average Score\n",
      "P P\n",
      "N h\n",
      "\n",
      "P\n",
      "a\n",
      "\n",
      " \n",
      "\n",
      "Non-Art Art\n",
      "\n",
      "(3) (b)\n",
      "1 semantic 'miIarity B32 1 CLIP Score B32\n",
      "1 semantic similarity L14 1 CLIP Score L14\n",
      "\n",
      " \n",
      "\n",
      "Figure 24: Semantic similarity and CLIP score for partic-\n",
      "ipants with and without art background, in (a) controlled\n",
      "dataset, and (b) uncontrolled dataset.\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "40.0 ,0.95 ‘\n",
      "E E ‘’ W 3 --\n",
      "8 37's E 0.90 if I ——\n",
      "\"' \"E 3\n",
      "1: 35.0 G 0 75 3\n",
      "g 32.5 E 1\n",
      "§30.0 §0 70 3\n",
      "E 27 5 E ’e’ 3 --\n",
      "3 ' E 0 55 E if 1 ——\n",
      "325.0 I 0 go so 5 \n",
      "\n",
      "—— . ., 5 1\n",
      "322.5 3 : 5 ' 1\n",
      "< D 5 >\n",
      "\n",
      "20.0 ‘ 0.55\n",
      "Non-Art Art\n",
      "(3) ( )\n",
      "\n",
      "Z Controlled Z Uncontrolled\n",
      "\n",
      "Figure 25: Image hash score and perceptual similarity for\n",
      "participants with and without art background, in (a) con-\n",
      "trolled dataset, and (b) uncontrolled dataset.\n",
      "\n",
      "   \n",
      "\n",
      " \n",
      "\n",
      "1 0 . 1 0\n",
      "0.9 ‘‘ 1\n",
      "cl a 3 0\n",
      ". 1\n",
      "lg 0 6 + 3\n",
      "at 3 3\n",
      "°' 1 9\n",
      "E 0.4 1 a\n",
      "3 0 1\n",
      "< 1\n",
      "0.2 1\n",
      "_ _ 0.0 _ _ ‘\n",
      "Unlverslty ivrmrk Unlverslty Mmrk\n",
      "(3) (b)\n",
      "\n",
      "1 semantic similarity B32 1 CLIP Score B32\n",
      "1 semantic 5 arity L14 1 CLIP Score L14\n",
      "\n",
      " \n",
      "\n",
      "Figure 26: Semantic similarity and CLIP score for partici-\n",
      "pants recruited on campus vs. on MTurk, in (a) controlled\n",
      "dataset, and (b) uncontrolled dataset.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "40.0 ‘\n",
      "$37.5 —— __  if P’\n",
      "Q 1\n",
      "1: 35.0 G 0 75 \n",
      "g 32.5 E -  -\n",
      ":g'30.0 E-070 \n",
      "E . 1\n",
      "i::::  2  0 ——\n",
      "g 50.50 D 3 a o\n",
      "5 22.5 E \n",
      "¢ 1\n",
      "\n",
      " \n",
      "\n",
      "20.0\n",
      "\n",
      "P\n",
      "in\n",
      "in\n",
      "\n",
      "University MTurk University MTurk\n",
      "\n",
      "Z Controlled Z Uncontrolled\n",
      "\n",
      "Figure 27: Image hash score and perceptual similarity for\n",
      "participants recruited on campus vs. on MTurk, in (a) con-\n",
      "trolled dataset, and (b) uncontrolled dataset.\n",
      "\n",
      "0.90 0.90\n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      " \n",
      "\n",
      "E W’ E ...\n",
      "E 0.95 E 0.95\n",
      "III 0.90 III 0.90\n",
      "§ 0.75 E 0.75\n",
      "E 0.70 E 0.70\n",
      "2 0.55 3 0.55\n",
      "€0.50 ‘’ ° o €0.50\n",
      "E 0.55 E 0.55 ‘’\n",
      "\"'5\" Nut siigmiy St.\\me- Very 0'5\"’ Nut siigmiy St.\\me- Very\n",
      "It Ill what It Ill what\n",
      "(8) (b)\n",
      "\n",
      "a\n",
      "3 36 B‘ 0.95\n",
      "U D 9\n",
      "\"' 34 0 90\n",
      ".:\n",
      "\"I 32 in\n",
      "II\n",
      "\n",
      "— 0 75\n",
      "i so 2\n",
      "E25 § 0 70\n",
      "\n",
      "2 . s . ,\n",
      "Z 26 g 0 55 as a 9 a S\n",
      "E 24 g 0 0 B U 0 Z\n",
      "\" o 0 u o\n",
      "g 22 E 0 50\n",
      "4 3\n",
      "\n",
      "2° Not siigmiy Some- Very “ \"'55 Nut siigmiy Sume- Very\n",
      "it ill what It Ill what\n",
      "\n",
      "(0) (d)\n",
      "\n",
      "1 Text 1 Audio 1 Video 1 Image\n",
      "\n",
      "Figure 28: Prompt inference accuracy measured for partici-\n",
      "pants with different levels of AI tools familiarity, using (a,b)\n",
      "B32 and L14 CLIP scores, respectively, (c) image hash, and\n",
      "(d) perceptual similarity, in the controlled dataset.\n",
      "\n",
      "    \n",
      "\n",
      "         \n",
      "\n",
      " \n",
      "\n",
      "E ,,_g,, E 0.90\n",
      "a a\n",
      "g ‘ms : 0.95\n",
      "3 9 0.90\n",
      "H 0.90 5 \"J5\n",
      "A. A.\n",
      ": “-75 : 0.70\n",
      ":4 u\n",
      "., 0.70 v 0.55\n",
      "D D\n",
      "E 0.55 E 0-60\n",
      "¥ ¥ 0 55\n",
      "< 0.50 < '\n",
      "\n",
      "0.50\n",
      "\n",
      "Nut siigmiy St.\\me- Very Nut siigmiy Sume-\n",
      "IQ Ill WHII It Ill whit\n",
      "(8) (b)\n",
      ",, 40.0 _r}‘ 0.95\n",
      "E 375 o 5 9 0 5\n",
      "g 0 D O 0.90\n",
      ".: 35.0 -\n",
      "E — 0.75\n",
      ": 32.5 g\n",
      "§30.0 § 0.70\n",
      "E 27.5 E\n",
      ".,   1°55 5'’\n",
      "325.0 .,\n",
      "D .. u .\n",
      "E 22.5 s 0 u E \"'5\"\n",
      "> v\n",
      "“ 20.0 ° ° D D ’ 0.55\n",
      "Nut siigmiy St.\\me- Very “ Nut siigmiy St.\\me- Very\n",
      "It Ill WHII It Ill whit\n",
      "(0) (d)\n",
      "\n",
      "1 Text 1 Audio 1 Video 1 Image\n",
      "\n",
      "Figure 29: Prompt inference accuracy measured for partici-\n",
      "pants with different levels of AI tools familiarity, using (a,b)\n",
      "B32 and L14 CLIP scores, respectively, (c) image hash, and\n",
      "(d) perceptual similarity, in the uncontrolled dataset.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pdf2image\n",
    "text = pytesseract.image_to_string(image='Files/Testing/Image/abstract.jpg')\n",
    "\n",
    "path = \"Files/Testing/pdf/2410.08406v1.pdf\"\n",
    "\n",
    "try:\n",
    "    from PIL import Image\n",
    "except ImportError:\n",
    "    import Image\n",
    "import pytesseract\n",
    "\n",
    "\n",
    "def pdf_to_img(pdf_file):\n",
    "    return pdf2image.convert_from_path(pdf_file)\n",
    "\n",
    "\n",
    "def ocr_core(file):\n",
    "    text = pytesseract.image_to_string(file)\n",
    "    return text\n",
    "\n",
    "\n",
    "def print_pages(pdf_file):\n",
    "    images = pdf_to_img(pdf_file)\n",
    "    for pg, img in enumerate(images):\n",
    "        print(ocr_core(img))\n",
    "\n",
    "\n",
    "print_pages(path)\n",
    "\n",
    "\n",
    "#text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31931397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = 'Files/Testing/pdf/medium.pdf'\n",
    "\n",
    "loader = PyPDFLoader(file_path=file_path)\n",
    "documents = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "\n",
    "split_docs = splitter.split_documents(documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ca68b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\markereversey\\AppData\\Local\\Temp\\ipykernel_16960\\1812023079.py:13: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# สร้าง embedding model (ไม่ใช้ OpenAI ก็ได้)\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=embedding,\n",
    "    persist_directory=\"medium_db\"  # โฟลเดอร์เก็บข้อมูล\n",
    ")\n",
    "\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "962d662e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma(\n",
    "    persist_directory=\"medium_db\",\n",
    "    embedding_function=embedding\n",
    ")\n",
    "\n",
    "query = \"บทความนี้เกี่ยวกับอะไร\"\n",
    "results = vectordb.similarity_search(query, k=5)\n",
    "\n",
    "#for doc in results:\n",
    "#    print(f\"[หน้า: {doc.metadata.get('page', '?')}]\")\n",
    "#    print(doc.page_content)\n",
    "#    print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5880726a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'การ Standardization เป็นขั้นตอนแรกในการท างานของ Principal Component Analysis (PCA) และเป็นสิ่งสำคัญอย่างยิ่งต่อกระบวนการนี้ การทำตามขั้นตอนนี้ช่วยให้แต่ละตัวแปรใน Dataset มี Range ที่เท่ากัน ซึ่งช่วยป้องกันผลกระทบของตัวแปรที่มี Range กว้างกว่า และทำให้การวิเคราะห์ง่ายขึ้น'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb = Chroma(\n",
    "    persist_directory=\"medium_db\",\n",
    "    embedding_function=embedding\n",
    ")\n",
    "\n",
    "def generate_res(query):\n",
    "    results = vectordb.similarity_search(query, k=5)\n",
    "\n",
    "    context = \"\\n\".join([doc.page_content for doc in results])\n",
    "\n",
    "    prompt = f\"Answer the question based on the following context:\\n{context}\\n\\nQuestion\"\n",
    "\n",
    "    response = ollama.chat(model=\"llama3.2\", messages=[\n",
    "        {\"role\":\"system\", \"content\":\"You are an assistant.\"},\n",
    "        {\"role\":\"user\", \"content\": prompt}\n",
    "    ])\n",
    "\n",
    "    return response[\"message\"][\"content\"]\n",
    "generate_res(\"standardization คืออะไร\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77cb20e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
